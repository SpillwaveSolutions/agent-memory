---
phase: 01-foundation
plan: 01
type: execute
wave: 2
depends_on: ["01-00"]
files_modified:
  - crates/memory-storage/src/lib.rs
  - crates/memory-storage/src/db.rs
  - crates/memory-storage/src/keys.rs
  - crates/memory-storage/src/column_families.rs
  - crates/memory-storage/src/error.rs
autonomous: true

must_haves:
  truths:
    - "RocksDB opens with 6 column families"
    - "Events can be written with time-prefixed keys"
    - "Events can be retrieved by exact key or time range"
    - "Write batches atomically commit event + outbox entry"
  artifacts:
    - path: "crates/memory-storage/src/db.rs"
      provides: "RocksDB wrapper with open/close/write/read"
      exports: ["Storage"]
    - path: "crates/memory-storage/src/keys.rs"
      provides: "Key encoding/decoding for time-prefixed keys"
      exports: ["EventKey", "OutboxKey"]
    - path: "crates/memory-storage/src/column_families.rs"
      provides: "Column family constants and options"
      exports: ["CF_EVENTS", "CF_TOC_NODES", "CF_OUTBOX"]
    - path: "crates/memory-storage/src/error.rs"
      provides: "Storage-specific error types"
      exports: ["StorageError"]
  key_links:
    - from: "crates/memory-storage/src/db.rs"
      to: "crates/memory-storage/src/column_families.rs"
      via: "imports CF constants"
      pattern: "use.*column_families"
    - from: "crates/memory-storage/src/db.rs"
      to: "crates/memory-storage/src/keys.rs"
      via: "uses key encoding"
      pattern: "use.*keys"
---

<objective>
Implement the RocksDB storage layer with column family isolation, time-prefixed keys, and atomic write batches.

Purpose: Enable event persistence with efficient time-range queries, which is the foundation for all data storage in the system.
Output: Working Storage struct that can open/close RocksDB, write events atomically with outbox entries, and read events by key or range.
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-00-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create column families module and storage error types</name>
  <files>
    - crates/memory-storage/src/column_families.rs
    - crates/memory-storage/src/error.rs
  </files>
  <action>
Create column family definitions with proper compaction settings per STOR-02 and STOR-05.

**crates/memory-storage/src/column_families.rs:**
```rust
//! Column family definitions for RocksDB.
//!
//! Each column family isolates data with different access patterns:
//! - events: Append-only conversation events (Universal compaction)
//! - toc_nodes: TOC hierarchy nodes (default compaction)
//! - toc_latest: Latest TOC node version pointers (default compaction)
//! - grips: Excerpt-to-event links (default compaction)
//! - outbox: Queue for async index updates (FIFO compaction)
//! - checkpoints: Crash recovery checkpoints (default compaction)

use rocksdb::{ColumnFamilyDescriptor, Options};

/// Column family name for conversation events
pub const CF_EVENTS: &str = "events";

/// Column family name for TOC hierarchy nodes
pub const CF_TOC_NODES: &str = "toc_nodes";

/// Column family name for latest TOC node version pointers
pub const CF_TOC_LATEST: &str = "toc_latest";

/// Column family name for grips (excerpt + event pointers)
pub const CF_GRIPS: &str = "grips";

/// Column family name for outbox queue (async index updates)
pub const CF_OUTBOX: &str = "outbox";

/// Column family name for background job checkpoints
pub const CF_CHECKPOINTS: &str = "checkpoints";

/// All column family names
pub const ALL_CF_NAMES: &[&str] = &[
    CF_EVENTS,
    CF_TOC_NODES,
    CF_TOC_LATEST,
    CF_GRIPS,
    CF_OUTBOX,
    CF_CHECKPOINTS,
];

/// Create column family options for events (append-only, compressed)
fn events_options() -> Options {
    let mut opts = Options::default();
    // Zstd compression for space efficiency
    opts.set_compression_type(rocksdb::DBCompressionType::Zstd);
    opts
}

/// Create column family options for outbox (FIFO for queue behavior)
fn outbox_options() -> Options {
    let mut opts = Options::default();
    // FIFO compaction for queue-like workload per STOR-05
    opts.set_compaction_style(rocksdb::DBCompactionStyle::Fifo);
    // Set max table files size for FIFO (required)
    opts.set_fifo_compaction_options(&rocksdb::FifoCompactionOptions::default());
    opts
}

/// Build all column family descriptors
pub fn build_cf_descriptors() -> Vec<ColumnFamilyDescriptor> {
    vec![
        ColumnFamilyDescriptor::new(CF_EVENTS, events_options()),
        ColumnFamilyDescriptor::new(CF_TOC_NODES, Options::default()),
        ColumnFamilyDescriptor::new(CF_TOC_LATEST, Options::default()),
        ColumnFamilyDescriptor::new(CF_GRIPS, Options::default()),
        ColumnFamilyDescriptor::new(CF_OUTBOX, outbox_options()),
        ColumnFamilyDescriptor::new(CF_CHECKPOINTS, Options::default()),
    ]
}
```

**crates/memory-storage/src/error.rs:**
```rust
//! Storage layer error types.

use thiserror::Error;

/// Errors that can occur in the storage layer
#[derive(Error, Debug)]
pub enum StorageError {
    /// RocksDB operation failed
    #[error("RocksDB error: {0}")]
    RocksDb(#[from] rocksdb::Error),

    /// Column family not found
    #[error("Column family not found: {0}")]
    ColumnFamilyNotFound(String),

    /// Key encoding/decoding error
    #[error("Key error: {0}")]
    Key(String),

    /// Serialization/deserialization error
    #[error("Serialization error: {0}")]
    Serialization(String),

    /// Event not found
    #[error("Event not found: {0}")]
    NotFound(String),
}

impl From<serde_json::Error> for StorageError {
    fn from(err: serde_json::Error) -> Self {
        StorageError::Serialization(err.to_string())
    }
}
```
  </action>
  <verify>
`cargo check -p memory-storage` compiles without errors.
  </verify>
  <done>
Column family constants defined with proper compaction settings. StorageError enum covers all storage failure modes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create key encoding module for time-prefixed keys</name>
  <files>
    - crates/memory-storage/src/keys.rs
  </files>
  <action>
Implement time-prefixed key encoding per STOR-01 (format: `evt:{ts}:{ulid}`).

**crates/memory-storage/src/keys.rs:**
```rust
//! Key encoding and decoding for storage layer.
//!
//! Key format: `{prefix}:{timestamp_ms}:{ulid}`
//! - prefix: identifies the key type (evt, outbox, etc.)
//! - timestamp_ms: milliseconds since Unix epoch, zero-padded to 13 digits
//! - ulid: 26-character ULID for uniqueness within same millisecond
//!
//! This format enables efficient time-range scans via RocksDB prefix iteration.

use ulid::Ulid;
use crate::error::StorageError;

/// Key for event storage
/// Format: evt:{timestamp_ms:013}:{ulid}
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct EventKey {
    /// Source timestamp in milliseconds
    pub timestamp_ms: i64,
    /// Unique identifier (also serves as event_id)
    pub ulid: Ulid,
}

impl EventKey {
    /// Create a new event key with given timestamp and fresh ULID
    pub fn new(timestamp_ms: i64) -> Self {
        Self {
            timestamp_ms,
            ulid: Ulid::new(),
        }
    }

    /// Create an event key from existing timestamp and ULID
    pub fn from_parts(timestamp_ms: i64, ulid: Ulid) -> Self {
        Self { timestamp_ms, ulid }
    }

    /// Create an event key from an event_id string (the ULID portion)
    /// Uses the ULID's embedded timestamp
    pub fn from_event_id(event_id: &str) -> Result<Self, StorageError> {
        let ulid: Ulid = event_id.parse()
            .map_err(|e| StorageError::Key(format!("Invalid event_id ULID: {}", e)))?;
        // ULID contains timestamp - extract it
        let timestamp_ms = ulid.timestamp_ms() as i64;
        Ok(Self { timestamp_ms, ulid })
    }

    /// Encode key to bytes for storage
    /// Format: "evt:{timestamp_ms:013}:{ulid}"
    pub fn to_bytes(&self) -> Vec<u8> {
        // Zero-pad timestamp to 13 digits for lexicographic sorting
        format!("evt:{:013}:{}", self.timestamp_ms, self.ulid).into_bytes()
    }

    /// Decode key from bytes
    pub fn from_bytes(bytes: &[u8]) -> Result<Self, StorageError> {
        let s = std::str::from_utf8(bytes)
            .map_err(|e| StorageError::Key(format!("Invalid UTF-8: {}", e)))?;
        Self::from_str(s)
    }

    /// Parse from string format
    pub fn from_str(s: &str) -> Result<Self, StorageError> {
        let parts: Vec<&str> = s.split(':').collect();
        if parts.len() != 3 || parts[0] != "evt" {
            return Err(StorageError::Key(format!("Invalid event key format: {}", s)));
        }

        let timestamp_ms: i64 = parts[1].parse()
            .map_err(|e| StorageError::Key(format!("Invalid timestamp: {}", e)))?;
        let ulid: Ulid = parts[2].parse()
            .map_err(|e| StorageError::Key(format!("Invalid ULID: {}", e)))?;

        Ok(Self { timestamp_ms, ulid })
    }

    /// Get the event_id (ULID string) for this key
    pub fn event_id(&self) -> String {
        self.ulid.to_string()
    }

    /// Generate prefix for time range scan start
    pub fn prefix_start(start_ms: i64) -> Vec<u8> {
        format!("evt:{:013}:", start_ms).into_bytes()
    }

    /// Generate prefix for time range scan end (exclusive)
    pub fn prefix_end(end_ms: i64) -> Vec<u8> {
        format!("evt:{:013}:", end_ms).into_bytes()
    }
}

/// Key for outbox entries (async index updates)
/// Format: outbox:{sequence:020}
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct OutboxKey {
    /// Monotonic sequence number
    pub sequence: u64,
}

impl OutboxKey {
    /// Create a new outbox key with given sequence
    pub fn new(sequence: u64) -> Self {
        Self { sequence }
    }

    /// Encode key to bytes
    pub fn to_bytes(&self) -> Vec<u8> {
        format!("outbox:{:020}", self.sequence).into_bytes()
    }

    /// Decode key from bytes
    pub fn from_bytes(bytes: &[u8]) -> Result<Self, StorageError> {
        let s = std::str::from_utf8(bytes)
            .map_err(|e| StorageError::Key(format!("Invalid UTF-8: {}", e)))?;

        let parts: Vec<&str> = s.split(':').collect();
        if parts.len() != 2 || parts[0] != "outbox" {
            return Err(StorageError::Key(format!("Invalid outbox key format: {}", s)));
        }

        let sequence: u64 = parts[1].parse()
            .map_err(|e| StorageError::Key(format!("Invalid sequence: {}", e)))?;

        Ok(Self { sequence })
    }
}

/// Key for checkpoint entries
/// Format: checkpoint:{job_name}
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct CheckpointKey {
    /// Job name (e.g., "segmenter", "day_rollup")
    pub job_name: String,
}

impl CheckpointKey {
    pub fn new(job_name: impl Into<String>) -> Self {
        Self { job_name: job_name.into() }
    }

    pub fn to_bytes(&self) -> Vec<u8> {
        format!("checkpoint:{}", self.job_name).into_bytes()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_event_key_roundtrip() {
        let key = EventKey::new(1706540400000);
        let bytes = key.to_bytes();
        let decoded = EventKey::from_bytes(&bytes).unwrap();
        assert_eq!(key.timestamp_ms, decoded.timestamp_ms);
        assert_eq!(key.ulid, decoded.ulid);
    }

    #[test]
    fn test_event_key_lexicographic_order() {
        let key1 = EventKey::from_parts(1000, Ulid::new());
        let key2 = EventKey::from_parts(2000, Ulid::new());
        assert!(key1.to_bytes() < key2.to_bytes());
    }

    #[test]
    fn test_event_key_from_event_id() {
        let original = EventKey::new(1706540400000);
        let event_id = original.event_id();
        let reconstructed = EventKey::from_event_id(&event_id).unwrap();
        assert_eq!(original.ulid, reconstructed.ulid);
    }

    #[test]
    fn test_outbox_key_roundtrip() {
        let key = OutboxKey::new(12345);
        let bytes = key.to_bytes();
        let decoded = OutboxKey::from_bytes(&bytes).unwrap();
        assert_eq!(key.sequence, decoded.sequence);
    }
}
```
  </action>
  <verify>
`cargo test -p memory-storage` runs key encoding tests successfully.
  </verify>
  <done>
EventKey and OutboxKey implement proper encoding with time-prefix format. Tests verify roundtrip and lexicographic ordering.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement Storage struct with RocksDB operations</name>
  <files>
    - crates/memory-storage/src/db.rs
    - crates/memory-storage/src/lib.rs
  </files>
  <action>
Implement the main Storage struct with open, write, and read operations.

**crates/memory-storage/src/db.rs:**
```rust
//! RocksDB wrapper for agent-memory storage.
//!
//! Provides:
//! - Database open/close with column family setup
//! - Atomic write batches (event + outbox per ING-05)
//! - Single-key and range reads
//! - Idempotent writes (ING-03)

use rocksdb::{DB, Options, WriteBatch, IteratorMode, Direction};
use std::path::Path;
use std::sync::atomic::{AtomicU64, Ordering};
use tracing::{debug, info};

use crate::column_families::{build_cf_descriptors, ALL_CF_NAMES, CF_EVENTS, CF_OUTBOX, CF_CHECKPOINTS};
use crate::error::StorageError;
use crate::keys::{EventKey, OutboxKey, CheckpointKey};

/// Main storage interface for agent-memory
pub struct Storage {
    db: DB,
    /// Outbox sequence counter for monotonic ordering
    outbox_sequence: AtomicU64,
}

impl Storage {
    /// Open storage at the given path, creating if necessary
    ///
    /// Per STOR-04: Each project gets its own RocksDB instance.
    /// Per STOR-05: Uses Universal compaction for append-only workload.
    pub fn open(path: &Path) -> Result<Self, StorageError> {
        info!("Opening storage at {:?}", path);

        let mut db_opts = Options::default();
        db_opts.create_if_missing(true);
        db_opts.create_missing_column_families(true);
        // Universal compaction for append-only (STOR-05)
        db_opts.set_compaction_style(rocksdb::DBCompactionStyle::Universal);
        // Limit memory usage during compaction
        db_opts.set_max_background_jobs(4);

        let cf_descriptors = build_cf_descriptors();
        let db = DB::open_cf_descriptors(&db_opts, path, cf_descriptors)?;

        // Initialize outbox sequence from highest existing key
        let outbox_sequence = Self::load_outbox_sequence(&db)?;

        Ok(Self {
            db,
            outbox_sequence: AtomicU64::new(outbox_sequence),
        })
    }

    /// Load the highest outbox sequence number from storage
    fn load_outbox_sequence(db: &DB) -> Result<u64, StorageError> {
        let cf = db.cf_handle(CF_OUTBOX)
            .ok_or_else(|| StorageError::ColumnFamilyNotFound(CF_OUTBOX.to_string()))?;

        // Iterate in reverse to find highest key
        let mut iter = db.iterator_cf(&cf, IteratorMode::End);
        if let Some(result) = iter.next() {
            let (key, _) = result?;
            let outbox_key = OutboxKey::from_bytes(&key)?;
            return Ok(outbox_key.sequence + 1);
        }
        Ok(0)
    }

    /// Get next outbox sequence number
    fn next_outbox_sequence(&self) -> u64 {
        self.outbox_sequence.fetch_add(1, Ordering::SeqCst)
    }

    /// Store an event with atomic outbox entry (ING-05)
    ///
    /// Returns (event_key, created) where created=false if event already existed (ING-03 idempotent)
    pub fn put_event(
        &self,
        event_id: &str,
        event_bytes: &[u8],
        outbox_bytes: &[u8],
    ) -> Result<(EventKey, bool), StorageError> {
        let events_cf = self.db.cf_handle(CF_EVENTS)
            .ok_or_else(|| StorageError::ColumnFamilyNotFound(CF_EVENTS.to_string()))?;
        let outbox_cf = self.db.cf_handle(CF_OUTBOX)
            .ok_or_else(|| StorageError::ColumnFamilyNotFound(CF_OUTBOX.to_string()))?;

        // Parse event_id to get key (ING-03: idempotent using event_id)
        let event_key = EventKey::from_event_id(event_id)?;

        // Check if already exists (idempotent)
        if self.db.get_cf(&events_cf, event_key.to_bytes())?.is_some() {
            debug!("Event {} already exists, skipping", event_id);
            return Ok((event_key, false));
        }

        // Atomic write: event + outbox entry
        let outbox_key = OutboxKey::new(self.next_outbox_sequence());

        let mut batch = WriteBatch::default();
        batch.put_cf(&events_cf, event_key.to_bytes(), event_bytes);
        batch.put_cf(&outbox_cf, outbox_key.to_bytes(), outbox_bytes);

        self.db.write(batch)?;
        debug!("Stored event {} with outbox seq {}", event_id, outbox_key.sequence);

        Ok((event_key, true))
    }

    /// Get an event by its event_id
    pub fn get_event(&self, event_id: &str) -> Result<Option<Vec<u8>>, StorageError> {
        let events_cf = self.db.cf_handle(CF_EVENTS)
            .ok_or_else(|| StorageError::ColumnFamilyNotFound(CF_EVENTS.to_string()))?;

        let event_key = EventKey::from_event_id(event_id)?;
        let result = self.db.get_cf(&events_cf, event_key.to_bytes())?;
        Ok(result)
    }

    /// Get events in a time range [start_ms, end_ms)
    ///
    /// Returns Vec<(EventKey, bytes)> ordered by time.
    pub fn get_events_in_range(
        &self,
        start_ms: i64,
        end_ms: i64,
    ) -> Result<Vec<(EventKey, Vec<u8>)>, StorageError> {
        let events_cf = self.db.cf_handle(CF_EVENTS)
            .ok_or_else(|| StorageError::ColumnFamilyNotFound(CF_EVENTS.to_string()))?;

        let start_prefix = EventKey::prefix_start(start_ms);
        let end_prefix = EventKey::prefix_end(end_ms);

        let mut results = Vec::new();
        let iter = self.db.iterator_cf(
            &events_cf,
            IteratorMode::From(&start_prefix, Direction::Forward),
        );

        for item in iter {
            let (key, value) = item?;
            // Stop if we've passed the end prefix
            if key.as_ref() >= end_prefix.as_slice() {
                break;
            }
            let event_key = EventKey::from_bytes(&key)?;
            results.push((event_key, value.to_vec()));
        }

        Ok(results)
    }

    /// Store a checkpoint for crash recovery (STOR-03)
    pub fn put_checkpoint(&self, job_name: &str, checkpoint_bytes: &[u8]) -> Result<(), StorageError> {
        let cf = self.db.cf_handle(CF_CHECKPOINTS)
            .ok_or_else(|| StorageError::ColumnFamilyNotFound(CF_CHECKPOINTS.to_string()))?;

        let key = CheckpointKey::new(job_name);
        self.db.put_cf(&cf, key.to_bytes(), checkpoint_bytes)?;
        Ok(())
    }

    /// Get a checkpoint for crash recovery (STOR-03)
    pub fn get_checkpoint(&self, job_name: &str) -> Result<Option<Vec<u8>>, StorageError> {
        let cf = self.db.cf_handle(CF_CHECKPOINTS)
            .ok_or_else(|| StorageError::ColumnFamilyNotFound(CF_CHECKPOINTS.to_string()))?;

        let key = CheckpointKey::new(job_name);
        let result = self.db.get_cf(&cf, key.to_bytes())?;
        Ok(result)
    }

    /// Flush all column families to disk
    pub fn flush(&self) -> Result<(), StorageError> {
        for cf_name in ALL_CF_NAMES {
            if let Some(cf) = self.db.cf_handle(cf_name) {
                self.db.flush_cf(&cf)?;
            }
        }
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    fn create_test_storage() -> (Storage, TempDir) {
        let temp_dir = TempDir::new().unwrap();
        let storage = Storage::open(temp_dir.path()).unwrap();
        (storage, temp_dir)
    }

    #[test]
    fn test_open_creates_column_families() {
        let (storage, _temp) = create_test_storage();
        // Verify all CFs exist by trying to get handles
        for cf_name in ALL_CF_NAMES {
            assert!(storage.db.cf_handle(cf_name).is_some(), "CF {} should exist", cf_name);
        }
    }

    #[test]
    fn test_put_and_get_event() {
        let (storage, _temp) = create_test_storage();

        let event_id = ulid::Ulid::new().to_string();
        let event_bytes = b"test event data";
        let outbox_bytes = b"outbox entry";

        let (key, created) = storage.put_event(&event_id, event_bytes, outbox_bytes).unwrap();
        assert!(created);
        assert_eq!(key.event_id(), event_id);

        let retrieved = storage.get_event(&event_id).unwrap();
        assert_eq!(retrieved, Some(event_bytes.to_vec()));
    }

    #[test]
    fn test_idempotent_put() {
        let (storage, _temp) = create_test_storage();

        let event_id = ulid::Ulid::new().to_string();
        let event_bytes = b"test event data";
        let outbox_bytes = b"outbox entry";

        let (_, created1) = storage.put_event(&event_id, event_bytes, outbox_bytes).unwrap();
        let (_, created2) = storage.put_event(&event_id, event_bytes, outbox_bytes).unwrap();

        assert!(created1);
        assert!(!created2); // Second write should be idempotent
    }

    #[test]
    fn test_get_events_in_range() {
        let (storage, _temp) = create_test_storage();

        // Create events at different timestamps
        let ts1 = 1000i64;
        let ts2 = 2000i64;
        let ts3 = 3000i64;

        let ulid1 = ulid::Ulid::from_parts(ts1 as u64, rand::random());
        let ulid2 = ulid::Ulid::from_parts(ts2 as u64, rand::random());
        let ulid3 = ulid::Ulid::from_parts(ts3 as u64, rand::random());

        storage.put_event(&ulid1.to_string(), b"event1", b"outbox1").unwrap();
        storage.put_event(&ulid2.to_string(), b"event2", b"outbox2").unwrap();
        storage.put_event(&ulid3.to_string(), b"event3", b"outbox3").unwrap();

        // Query range [1500, 2500) should only get event2
        let results = storage.get_events_in_range(1500, 2500).unwrap();
        assert_eq!(results.len(), 1);
        assert_eq!(results[0].1, b"event2");
    }

    #[test]
    fn test_checkpoint_roundtrip() {
        let (storage, _temp) = create_test_storage();

        let job_name = "test_job";
        let checkpoint_data = b"checkpoint state";

        storage.put_checkpoint(job_name, checkpoint_data).unwrap();
        let retrieved = storage.get_checkpoint(job_name).unwrap();

        assert_eq!(retrieved, Some(checkpoint_data.to_vec()));
    }
}
```

**Update crates/memory-storage/src/lib.rs:**
```rust
//! Storage layer for agent-memory system.
//!
//! Provides RocksDB-backed storage with:
//! - Column family isolation for different data types (STOR-02)
//! - Time-prefixed keys for efficient range scans (STOR-01)
//! - Atomic writes via WriteBatch (ING-05)
//! - Idempotent event writes (ING-03)
//! - Checkpoint-based crash recovery (STOR-03)

pub mod column_families;
pub mod db;
pub mod error;
pub mod keys;

pub use db::Storage;
pub use error::StorageError;
pub use keys::{EventKey, OutboxKey, CheckpointKey};
```

Add `rand` to dev-dependencies for tests in `crates/memory-storage/Cargo.toml`:
```toml
[dev-dependencies]
tempfile = "3"
rand = "0.8"
```
  </action>
  <verify>
`cargo test -p memory-storage` passes all tests including:
- Column family creation
- Event put/get roundtrip
- Idempotent writes
- Time range queries
- Checkpoint storage
  </verify>
  <done>
Storage struct opens RocksDB with 6 column families, supports atomic event+outbox writes, idempotent puts, and time-range queries. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `cargo build -p memory-storage` compiles without errors
2. `cargo test -p memory-storage` passes all tests
3. Storage can:
   - Open RocksDB with 6 column families
   - Write events with time-prefixed keys
   - Read events by exact event_id
   - Query events by time range
   - Write/read checkpoints
   - Handle idempotent writes correctly
</verification>

<success_criteria>
- RocksDB opens with 6 column families (events, toc_nodes, toc_latest, grips, outbox, checkpoints)
- Events stored with time-prefixed keys (evt:{ts}:{ulid})
- Atomic write batches commit event + outbox entry together
- Idempotent writes return created=false for duplicates
- Time range queries return events in order
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
