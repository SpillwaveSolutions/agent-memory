---
id: 15-02
name: memory-llm skill
phase: 15
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - plugins/memory-setup-plugin/skills/memory-llm/SKILL.md
  - plugins/memory-setup-plugin/skills/memory-llm/references/provider-comparison.md
  - plugins/memory-setup-plugin/skills/memory-llm/references/model-selection.md
  - plugins/memory-setup-plugin/skills/memory-llm/references/cost-estimation.md
  - plugins/memory-setup-plugin/skills/memory-llm/references/custom-endpoints.md
autonomous: true
estimated_effort: M

must_haves:
  truths:
    - "User can select LLM provider via interactive wizard"
    - "User can discover available models for selected provider"
    - "User can test API connection before saving config"
    - "User can see cost estimation for selected model"
    - "User can configure quality/latency tradeoffs"
    - "User can set token budget optimization mode"
  artifacts:
    - path: "plugins/memory-setup-plugin/skills/memory-llm/SKILL.md"
      provides: "Interactive LLM wizard skill"
      min_lines: 250
    - path: "plugins/memory-setup-plugin/skills/memory-llm/references/provider-comparison.md"
      provides: "Provider comparison documentation"
      min_lines: 60
    - path: "plugins/memory-setup-plugin/skills/memory-llm/references/model-selection.md"
      provides: "Model selection guide"
      min_lines: 50
    - path: "plugins/memory-setup-plugin/skills/memory-llm/references/cost-estimation.md"
      provides: "Cost estimation documentation"
      min_lines: 40
    - path: "plugins/memory-setup-plugin/skills/memory-llm/references/custom-endpoints.md"
      provides: "Custom endpoint configuration"
      min_lines: 30
  key_links:
    - from: "plugins/memory-setup-plugin/skills/memory-llm/SKILL.md"
      to: "~/.config/memory-daemon/config.toml"
      via: "config generation for [summarizer] section"
      pattern: "\\[summarizer\\]"
    - from: "plugins/memory-setup-plugin/skills/memory-llm/SKILL.md"
      to: "OpenAI/Anthropic/Ollama APIs"
      via: "API test calls"
      pattern: "curl.*api\\.openai\\.com|api\\.anthropic\\.com|localhost:11434"
---

<objective>
Create the `/memory-llm` interactive wizard skill for deep LLM configuration including provider selection, model discovery, API testing, cost estimation, and quality/budget tuning.

Purpose: Provide users with guided LLM configuration that goes beyond the basic provider selection in `/memory-setup`, enabling model discovery, live API testing, and cost-aware decisions.

Output: SKILL.md file with 7-step wizard flow plus 4 reference documentation files.
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-configuration-wizard-skills/15-RESEARCH.md
@docs/plans/configuration-wizard-skills-plan.md
@plugins/memory-setup-plugin/skills/memory-setup/SKILL.md
@plugins/memory-setup-plugin/skills/memory-setup/references/wizard-questions.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create memory-llm SKILL.md with 7-step wizard</name>
  <files>plugins/memory-setup-plugin/skills/memory-llm/SKILL.md</files>
  <action>
Create the SKILL.md file following the memory-setup/SKILL.md pattern.

**YAML Frontmatter:**
```yaml
---
name: memory-llm
description: |
  This skill should be used when the user asks to "configure LLM",
  "change summarizer provider", "test API connection", "estimate LLM costs",
  "discover models", or "tune summarization quality". Provides interactive wizard
  for LLM provider configuration with model discovery and API testing.
license: MIT
metadata:
  version: 1.0.0
  author: SpillwaveSolutions
---
```

**Content Structure:**
1. Header section with purpose and when not to use
2. Quick Start table with commands:
   - `/memory-llm` - Interactive LLM wizard
   - `/memory-llm --test` - Test current API key only
   - `/memory-llm --discover` - List available models
   - `/memory-llm --estimate` - Show cost estimation

3. Question Flow diagram (ASCII art showing 7 steps)

4. State Detection section with bash commands:
   ```bash
   # API keys set?
   [ -n "$OPENAI_API_KEY" ] && echo "OPENAI: set" || echo "OPENAI: not set"
   [ -n "$ANTHROPIC_API_KEY" ] && echo "ANTHROPIC: set" || echo "ANTHROPIC: not set"

   # Current config
   grep -A10 '\[summarizer\]' ~/.config/memory-daemon/config.toml

   # Test connectivity
   curl -s -o /dev/null -w "%{http_code}" -H "Authorization: Bearer $OPENAI_API_KEY" https://api.openai.com/v1/models

   # Check Ollama
   curl -s http://localhost:11434/api/tags
   ```

5. **Seven wizard steps with AskUserQuestion format:**

**Step 1: Provider Selection**
- question: "Which LLM provider should generate summaries?"
- header: "Provider"
- options: OpenAI (Recommended), Anthropic, Ollama (Local), None
- Always ask (core decision) unless --minimal

**Step 2: Model Discovery**
- question: "Which model should be used for summarization?"
- header: "Model"
- options: Dynamic based on provider
  - OpenAI: gpt-4o-mini (Recommended, $0.15/1M), gpt-4o ($5/1M), gpt-4-turbo ($10/1M)
  - Anthropic: claude-3-5-haiku-latest (Recommended), claude-3-5-sonnet-latest
  - Ollama: List from `curl localhost:11434/api/tags`
- Show discovered models with pricing

**Step 3: API Key**
- question: "How should the API key be configured?"
- header: "API Key"
- options: Use existing environment variable (Recommended), Enter new key, Test existing key
- Skip if: env var set AND provider matches AND not --fresh

**Step 4: Test Connection**
- Live API test (not a question)
- Show result: [check] Connected or [x] Failed with error
- Offer retry or skip

**Step 5: Cost Estimation**
- Informational display (not a question)
- Show: "Based on typical usage (~1000 events/day), estimated cost: $X.XX/month"
- Include token calculation

**Step 6: Quality/Latency Tradeoffs** (--advanced only)
- question: "Configure quality vs latency tradeoff?"
- header: "Quality"
- options: Balanced (temp=0.3, max_tokens=512), Deterministic (temp=0.0), Creative (temp=0.7), Custom
- Skip in --minimal mode

**Step 7: Token Budget** (--advanced only)
- question: "Configure token budget optimization?"
- header: "Budget"
- options: Balanced (~$0.02/month), Economical (shorter summaries), Detailed (longer summaries), Custom
- Skip in --minimal mode

6. Config Generation section for [summarizer] config.toml
7. Validation section (key format, live test, model available)
8. Output Formatting with success/error displays
9. Reference Files links
10. Cross-skill navigation hints
  </action>
  <verify>
```bash
ls -la plugins/memory-setup-plugin/skills/memory-llm/SKILL.md
wc -l plugins/memory-setup-plugin/skills/memory-llm/SKILL.md
grep -c "Provider\|Model\|API Key" plugins/memory-setup-plugin/skills/memory-llm/SKILL.md
```
File exists, has 250+ lines, contains wizard steps
  </verify>
  <done>SKILL.md exists with 7-step wizard flow including model discovery and API testing</done>
</task>

<task type="auto">
  <name>Task 2: Create provider-comparison.md and model-selection.md references</name>
  <files>
plugins/memory-setup-plugin/skills/memory-llm/references/provider-comparison.md
plugins/memory-setup-plugin/skills/memory-llm/references/model-selection.md
  </files>
  <action>
**Create provider-comparison.md:**
1. Provider overview table:
   | Provider | Cost | Quality | Latency | Privacy |
   |----------|------|---------|---------|---------|
   | OpenAI | $$   | High    | Fast    | Cloud   |
   | Anthropic | $$$ | Highest | Medium | Cloud   |
   | Ollama | Free | Variable| Slow    | Local   |
   | None | Free | N/A | N/A | N/A |

2. Detailed provider sections:
   - **OpenAI:** GPT models, API key from platform.openai.com, fast and reliable
   - **Anthropic:** Claude models, higher quality, API key from console.anthropic.com
   - **Ollama:** Local models, no API costs, requires local resources
   - **None:** Disables summarization, TOC only mode

3. When to choose each provider
4. API key management recommendations

**Create model-selection.md:**
1. Model comparison by provider
2. OpenAI models table:
   | Model | Price/1M tokens | Context | Best For |
   |-------|-----------------|---------|----------|
   | gpt-4o-mini | $0.15 | 128k | Most users (recommended) |
   | gpt-4o | $5.00 | 128k | Highest quality |
   | gpt-4-turbo | $10.00 | 128k | Legacy support |

3. Anthropic models table
4. Ollama models (common ones: llama3.2:3b, mistral, phi)
5. Model discovery commands
6. Quality vs cost tradeoff guidance
  </action>
  <verify>
```bash
ls -la plugins/memory-setup-plugin/skills/memory-llm/references/provider-comparison.md
ls -la plugins/memory-setup-plugin/skills/memory-llm/references/model-selection.md
grep -c "OpenAI\|Anthropic\|Ollama" plugins/memory-setup-plugin/skills/memory-llm/references/*.md
```
Both files exist with provider documentation
  </verify>
  <done>provider-comparison.md and model-selection.md created with detailed comparisons</done>
</task>

<task type="auto">
  <name>Task 3: Create cost-estimation.md and custom-endpoints.md references</name>
  <files>
plugins/memory-setup-plugin/skills/memory-llm/references/cost-estimation.md
plugins/memory-setup-plugin/skills/memory-llm/references/custom-endpoints.md
  </files>
  <action>
**Create cost-estimation.md:**
1. Cost calculation formula:
   - Tokens per summary: ~200-500
   - Summaries per day: depends on conversation volume
   - Monthly cost = (tokens/summary * summaries/day * 30) / 1,000,000 * price_per_1M

2. Usage tiers table:
   | Usage | Events/Day | Summaries | Monthly Cost (gpt-4o-mini) |
   |-------|------------|-----------|---------------------------|
   | Light | 100 | ~5 | $0.01 |
   | Medium | 1000 | ~50 | $0.10 |
   | Heavy | 5000 | ~250 | $0.50 |

3. Budget optimization modes
4. Token counting explanation
5. Monitoring usage section

**Create custom-endpoints.md:**
1. When to use custom endpoints:
   - OpenAI-compatible APIs (Azure OpenAI, LocalAI, LM Studio)
   - Private deployments
   - Proxy servers

2. Configuration example:
   ```toml
   [summarizer]
   provider = "openai"  # Use OpenAI-compatible protocol
   api_endpoint = "https://your-custom-endpoint/v1"
   model = "your-model"
   ```

3. Azure OpenAI specific configuration
4. Local API server setup (LocalAI, LM Studio)
5. Testing custom endpoints
  </action>
  <verify>
```bash
ls -la plugins/memory-setup-plugin/skills/memory-llm/references/cost-estimation.md
ls -la plugins/memory-setup-plugin/skills/memory-llm/references/custom-endpoints.md
wc -l plugins/memory-setup-plugin/skills/memory-llm/references/*.md
```
All four reference files exist
  </verify>
  <done>cost-estimation.md and custom-endpoints.md created with usage and configuration guidance</done>
</task>

</tasks>

<verification>
Run these commands to verify the plan is complete:

```bash
# Verify directory structure
ls -la plugins/memory-setup-plugin/skills/memory-llm/
ls -la plugins/memory-setup-plugin/skills/memory-llm/references/

# Verify SKILL.md structure
grep -E "^---$|^name:|question:|header:|options:" plugins/memory-setup-plugin/skills/memory-llm/SKILL.md | head -40

# Verify reference docs
for f in plugins/memory-setup-plugin/skills/memory-llm/references/*.md; do
  echo "=== $f ===" && head -10 "$f"
done

# Count lines
wc -l plugins/memory-setup-plugin/skills/memory-llm/SKILL.md
wc -l plugins/memory-setup-plugin/skills/memory-llm/references/*.md
```
</verification>

<success_criteria>
1. SKILL.md has valid YAML frontmatter with name: memory-llm
2. Seven wizard steps documented with AskUserQuestion format
3. Model discovery logic for each provider
4. API testing commands included
5. Four reference files created (provider, model, cost, endpoints)
6. Cost estimation formulas and tables
7. Custom endpoint configuration documented
</success_criteria>

<output>
After completion, create `.planning/phases/15-configuration-wizard-skills/15-02-SUMMARY.md`
</output>
