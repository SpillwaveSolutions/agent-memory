---
phase: 14-topic-graph-memory
plan: 02
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - crates/memory-topics/src/labeling.rs
  - crates/memory-topics/src/keywords.rs
  - crates/memory-topics/src/lib.rs
autonomous: true

must_haves:
  truths:
    - "Topics receive meaningful human-readable labels"
    - "LLM labeling produces concise 2-4 word topic names"
    - "Keyword fallback generates labels when LLM is unavailable"
    - "Labels are truncated to max 50 characters"
    - "Keywords are extracted from cluster summaries using term frequency"
  artifacts:
    - path: "crates/memory-topics/src/labeling.rs"
      provides: "Topic labeling with LLM and fallback"
      exports: ["TopicLabeler", "label_topic", "label_topics_batch"]
    - path: "crates/memory-topics/src/keywords.rs"
      provides: "Keyword extraction from text"
      exports: ["extract_keywords", "label_from_keywords", "STOPWORDS"]
  key_links:
    - from: "crates/memory-topics/src/labeling.rs"
      to: "memory-toc::summarizer"
      via: "reuses existing Summarizer trait"
      pattern: "Summarizer::summarize"
    - from: "crates/memory-topics/src/labeling.rs"
      to: "crates/memory-topics/src/keywords.rs"
      via: "fallback when LLM fails"
      pattern: "label_from_keywords"
---

<objective>
Implement topic labeling with LLM integration and keyword fallback.

Purpose: Transform clusters into meaningful topics by generating human-readable labels. LLM produces high-quality labels like "Authentication Security" while keyword extraction provides reliable fallback when LLM is unavailable or rate-limited.

Output: TopicLabeler with LLM integration, keyword extraction, and configurable fallback behavior.
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/14-topic-graph-memory/14-RESEARCH.md
@.planning/phases/14-topic-graph-memory/14-01-SUMMARY.md
@crates/memory-toc/src/summarizer.rs
@crates/memory-topics/src/config.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement keyword extraction</name>
  <files>
    crates/memory-topics/src/keywords.rs
    crates/memory-topics/src/lib.rs
  </files>
  <action>
Create keyword extraction utilities:

1. Create `crates/memory-topics/src/keywords.rs`:
   ```rust
   //! Keyword extraction from text.
   //!
   //! Simple term-frequency based extraction without external dependencies.

   use std::collections::{HashMap, HashSet};

   /// Common English stopwords to exclude from keyword extraction.
   pub static STOPWORDS: &[&str] = &[
       "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
       "of", "with", "by", "from", "as", "is", "was", "are", "were", "been",
       "be", "have", "has", "had", "do", "does", "did", "will", "would",
       "could", "should", "may", "might", "must", "this", "that", "these",
       "those", "it", "its", "we", "our", "you", "your", "about", "into",
       "through", "during", "before", "after", "above", "below", "between",
       "under", "again", "further", "then", "once", "here", "there", "when",
       "where", "why", "how", "all", "each", "few", "more", "most", "other",
       "some", "such", "no", "nor", "not", "only", "own", "same", "so",
       "than", "too", "very", "just", "can", "now", "also", "any", "both",
       "which", "what", "who", "whom", "while", "being", "using", "used",
   ];

   /// Get stopwords as a HashSet for efficient lookup.
   pub fn stopwords_set() -> HashSet<&'static str> {
       STOPWORDS.iter().copied().collect()
   }

   /// Extract top keywords from a collection of text summaries.
   ///
   /// Uses simple term frequency to identify important words.
   ///
   /// # Arguments
   /// * `summaries` - Collection of text strings to analyze
   /// * `top_n` - Number of top keywords to return
   ///
   /// # Returns
   /// Vector of keywords sorted by frequency (descending)
   pub fn extract_keywords(summaries: &[String], top_n: usize) -> Vec<String> {
       let stopwords = stopwords_set();
       let mut word_counts: HashMap<String, usize> = HashMap::new();

       for summary in summaries {
           for word in summary.split_whitespace() {
               let normalized = normalize_word(word);

               // Skip short words and stopwords
               if normalized.len() >= 3 && !stopwords.contains(normalized.as_str()) {
                   *word_counts.entry(normalized).or_insert(0) += 1;
               }
           }
       }

       // Sort by frequency descending
       let mut keywords: Vec<_> = word_counts.into_iter().collect();
       keywords.sort_by(|a, b| b.1.cmp(&a.1));

       keywords
           .into_iter()
           .take(top_n)
           .map(|(word, _)| word)
           .collect()
   }

   /// Extract keywords with their frequencies.
   pub fn extract_keywords_with_counts(
       summaries: &[String],
       top_n: usize,
   ) -> Vec<(String, usize)> {
       let stopwords = stopwords_set();
       let mut word_counts: HashMap<String, usize> = HashMap::new();

       for summary in summaries {
           for word in summary.split_whitespace() {
               let normalized = normalize_word(word);

               if normalized.len() >= 3 && !stopwords.contains(normalized.as_str()) {
                   *word_counts.entry(normalized).or_insert(0) += 1;
               }
           }
       }

       let mut keywords: Vec<_> = word_counts.into_iter().collect();
       keywords.sort_by(|a, b| b.1.cmp(&a.1));
       keywords.truncate(top_n);
       keywords
   }

   /// Normalize a word: lowercase, remove punctuation.
   fn normalize_word(word: &str) -> String {
       word.to_lowercase()
           .chars()
           .filter(|c| c.is_alphanumeric())
           .collect()
   }

   /// Generate a label from top keywords.
   ///
   /// Capitalizes each keyword and joins with space.
   ///
   /// # Arguments
   /// * `keywords` - Keywords to use for label
   /// * `max_words` - Maximum number of words in label (default 3)
   ///
   /// # Returns
   /// Label string like "Authentication Security Tokens"
   pub fn label_from_keywords(keywords: &[String], max_words: usize) -> String {
       keywords
           .iter()
           .take(max_words)
           .map(|k| capitalize(k))
           .collect::<Vec<_>>()
           .join(" ")
   }

   /// Capitalize first letter of a word.
   fn capitalize(s: &str) -> String {
       let mut chars = s.chars();
       match chars.next() {
           None => String::new(),
           Some(first) => first.to_uppercase().chain(chars).collect(),
       }
   }

   /// Truncate label to maximum length.
   pub fn truncate_label(label: &str, max_len: usize) -> String {
       if label.len() <= max_len {
           label.to_string()
       } else {
           // Truncate at word boundary if possible
           let truncated = &label[..max_len];
           match truncated.rfind(' ') {
               Some(pos) if pos > max_len / 2 => truncated[..pos].to_string(),
               _ => format!("{}...", &label[..max_len - 3]),
           }
       }
   }

   #[cfg(test)]
   mod tests {
       use super::*;

       #[test]
       fn test_extract_keywords() {
           let summaries = vec![
               "Discussed authentication and security tokens".to_string(),
               "Implemented JWT authentication for API security".to_string(),
               "Fixed authentication bug in token validation".to_string(),
           ];

           let keywords = extract_keywords(&summaries, 3);

           // "authentication" should be most common
           assert!(!keywords.is_empty());
           assert!(keywords.contains(&"authentication".to_string()));
       }

       #[test]
       fn test_extract_keywords_filters_stopwords() {
           let summaries = vec!["the quick brown fox and the lazy dog".to_string()];

           let keywords = extract_keywords(&summaries, 10);

           // Should not contain stopwords
           assert!(!keywords.contains(&"the".to_string()));
           assert!(!keywords.contains(&"and".to_string()));
           // Should contain content words
           assert!(keywords.contains(&"quick".to_string()));
           assert!(keywords.contains(&"brown".to_string()));
       }

       #[test]
       fn test_label_from_keywords() {
           let keywords = vec![
               "authentication".to_string(),
               "security".to_string(),
               "tokens".to_string(),
               "extra".to_string(),
           ];

           let label = label_from_keywords(&keywords, 3);
           assert_eq!(label, "Authentication Security Tokens");
       }

       #[test]
       fn test_capitalize() {
           assert_eq!(capitalize("hello"), "Hello");
           assert_eq!(capitalize("WORLD"), "WORLD");
           assert_eq!(capitalize(""), "");
       }

       #[test]
       fn test_truncate_label() {
           let long = "Authentication Security Implementation Details Extended";
           let truncated = truncate_label(long, 30);
           assert!(truncated.len() <= 30);

           let short = "Auth";
           assert_eq!(truncate_label(short, 30), "Auth");
       }

       #[test]
       fn test_normalize_word() {
           assert_eq!(normalize_word("Hello!"), "hello");
           assert_eq!(normalize_word("JWT's"), "jwts");
           assert_eq!(normalize_word("user-auth"), "userauth");
       }
   }
   ```

2. Update `crates/memory-topics/src/lib.rs` to export keywords module:
   ```rust
   // Add to existing exports:
   pub mod keywords;
   pub use keywords::{extract_keywords, label_from_keywords};
   ```
  </action>
  <verify>
    cd /Users/richardhightower/clients/spillwave/src/agent-memory && cargo test -p memory-topics keywords
  </verify>
  <done>
    Keyword extraction with stopword filtering. label_from_keywords generates labels. truncate_label handles length limits. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement topic labeling with LLM and fallback</name>
  <files>
    crates/memory-topics/src/labeling.rs
    crates/memory-topics/src/lib.rs
  </files>
  <action>
Create topic labeler with LLM integration:

1. Create `crates/memory-topics/src/labeling.rs`:
   ```rust
   //! Topic labeling with LLM and keyword fallback.
   //!
   //! Generates human-readable labels for topic clusters.

   use std::sync::Arc;

   use memory_toc::summarizer::Summarizer;
   use tracing::{debug, info, warn};

   use crate::config::LabelingConfig;
   use crate::error::TopicsError;
   use crate::extraction::ClusterResult;
   use crate::keywords::{extract_keywords, label_from_keywords, truncate_label};
   use crate::types::Topic;

   /// Prompt template for LLM labeling.
   const LABELING_PROMPT: &str = r#"You are a topic labeling assistant. Given summaries from a conversation cluster, generate a concise topic label.

   Requirements:
   - 2-4 words maximum
   - Descriptive and specific
   - No articles (a, an, the)
   - Title case

   Summaries:
   {summaries}

   Respond with ONLY the topic label, nothing else."#;

   /// Topic labeler that uses LLM with keyword fallback.
   pub struct TopicLabeler {
       config: LabelingConfig,
       summarizer: Option<Arc<dyn Summarizer>>,
   }

   impl TopicLabeler {
       /// Create a new topic labeler.
       ///
       /// # Arguments
       /// * `config` - Labeling configuration
       /// * `summarizer` - Optional LLM summarizer (if None, uses keyword fallback only)
       pub fn new(config: LabelingConfig, summarizer: Option<Arc<dyn Summarizer>>) -> Self {
           Self { config, summarizer }
       }

       /// Create labeler with keyword fallback only (no LLM).
       pub fn keywords_only(config: LabelingConfig) -> Self {
           Self {
               config,
               summarizer: None,
           }
       }

       /// Label a single cluster.
       pub async fn label_cluster(&self, cluster: &ClusterResult) -> Result<String, TopicsError> {
           // Extract keywords first (needed for both paths)
           let keywords = extract_keywords(&cluster.summaries, self.config.top_keywords);

           // Try LLM if configured and available
           if self.config.use_llm {
               if let Some(ref summarizer) = self.summarizer {
                   match self.label_with_llm(summarizer, &cluster.summaries).await {
                       Ok(label) => {
                           debug!(label = %label, "LLM labeling successful");
                           return Ok(truncate_label(&label, self.config.max_label_length));
                       }
                       Err(e) => {
                           warn!(error = %e, "LLM labeling failed, falling back to keywords");
                           if !self.config.fallback_to_keywords {
                               return Err(e);
                           }
                       }
                   }
               }
           }

           // Fall back to keyword-based labeling
           let label = if keywords.is_empty() {
               format!("Topic {}", cluster.label)
           } else {
               label_from_keywords(&keywords, 3)
           };

           Ok(truncate_label(&label, self.config.max_label_length))
       }

       /// Label a topic in place, also setting keywords.
       pub async fn label_topic(
           &self,
           topic: &mut Topic,
           cluster: &ClusterResult,
       ) -> Result<(), TopicsError> {
           // Extract and store keywords
           topic.keywords = extract_keywords(&cluster.summaries, self.config.top_keywords);

           // Generate label
           topic.label = self.label_cluster(cluster).await?;

           Ok(())
       }

       /// Label multiple topics in batch.
       pub async fn label_topics_batch(
           &self,
           topics: &mut [Topic],
           clusters: &[ClusterResult],
       ) -> Result<usize, TopicsError> {
           if topics.len() != clusters.len() {
               return Err(TopicsError::InvalidInput(
                   "Topics and clusters must have same length".to_string(),
               ));
           }

           let mut success_count = 0;

           for (topic, cluster) in topics.iter_mut().zip(clusters.iter()) {
               match self.label_topic(topic, cluster).await {
                   Ok(_) => success_count += 1,
                   Err(e) => {
                       warn!(topic_id = %topic.topic_id, error = %e, "Failed to label topic");
                       // Use placeholder label
                       topic.label = format!("Topic {}", cluster.label);
                   }
               }
           }

           info!(
               total = topics.len(),
               success = success_count,
               "Batch labeling complete"
           );
           Ok(success_count)
       }

       /// Label using LLM summarizer.
       async fn label_with_llm(
           &self,
           summarizer: &Arc<dyn Summarizer>,
           summaries: &[String],
       ) -> Result<String, TopicsError> {
           // Build prompt
           let summaries_text = summaries
               .iter()
               .take(5) // Limit context size
               .enumerate()
               .map(|(i, s)| format!("{}. {}", i + 1, truncate_summary(s, 200)))
               .collect::<Vec<_>>()
               .join("\n");

           let prompt = LABELING_PROMPT.replace("{summaries}", &summaries_text);

           // Call summarizer
           let response = summarizer
               .summarize(&prompt)
               .await
               .map_err(|e| TopicsError::Embedding(e.to_string()))?;

           // Parse response (should be just the label)
           let label = parse_label_response(&response);

           if label.is_empty() {
               return Err(TopicsError::Embedding("Empty label from LLM".to_string()));
           }

           Ok(label)
       }
   }

   /// Truncate summary for prompt.
   fn truncate_summary(summary: &str, max_len: usize) -> String {
       if summary.len() <= max_len {
           summary.to_string()
       } else {
           format!("{}...", &summary[..max_len - 3])
       }
   }

   /// Parse label from LLM response.
   fn parse_label_response(response: &str) -> String {
       // Clean up response: trim, remove quotes, limit length
       let label = response.trim().trim_matches('"').trim_matches('\'').trim();

       // Take only first line if multi-line
       let label = label.lines().next().unwrap_or("").trim();

       // Remove any markdown formatting
       let label = label.trim_start_matches('#').trim();

       label.to_string()
   }

   #[cfg(test)]
   mod tests {
       use super::*;

       fn make_cluster(label: i32, summaries: Vec<&str>) -> ClusterResult {
           ClusterResult {
               label,
               node_ids: vec!["n1".to_string()],
               embeddings: vec![vec![1.0]],
               summaries: summaries.into_iter().map(String::from).collect(),
           }
       }

       #[tokio::test]
       async fn test_label_cluster_keywords_only() {
           let config = LabelingConfig {
               use_llm: false,
               ..Default::default()
           };
           let labeler = TopicLabeler::keywords_only(config);

           let cluster = make_cluster(
               0,
               vec![
                   "Discussed authentication and security",
                   "Implemented JWT authentication",
                   "Fixed authentication bugs",
               ],
           );

           let label = labeler.label_cluster(&cluster).await.unwrap();
           assert!(!label.is_empty());
           // Should contain "Authentication" since it's most frequent
           assert!(label.to_lowercase().contains("authentication"));
       }

       #[tokio::test]
       async fn test_label_cluster_empty_summaries() {
           let config = LabelingConfig {
               use_llm: false,
               ..Default::default()
           };
           let labeler = TopicLabeler::keywords_only(config);

           let cluster = ClusterResult {
               label: 5,
               node_ids: vec![],
               embeddings: vec![],
               summaries: vec![],
           };

           let label = labeler.label_cluster(&cluster).await.unwrap();
           assert_eq!(label, "Topic 5");
       }

       #[test]
       fn test_parse_label_response() {
           assert_eq!(parse_label_response("Auth Security"), "Auth Security");
           assert_eq!(parse_label_response("  \"Auth Security\"  "), "Auth Security");
           assert_eq!(
               parse_label_response("Auth Security\nExtra line"),
               "Auth Security"
           );
           assert_eq!(parse_label_response("# Auth Security"), "Auth Security");
       }

       #[test]
       fn test_truncate_summary() {
           let short = "Short summary";
           assert_eq!(truncate_summary(short, 100), short);

           let long = "A".repeat(300);
           let truncated = truncate_summary(&long, 100);
           assert_eq!(truncated.len(), 100);
           assert!(truncated.ends_with("..."));
       }
   }
   ```

2. Update `crates/memory-topics/src/lib.rs` to export labeling module:
   ```rust
   // Add to existing exports:
   pub mod labeling;
   pub use labeling::TopicLabeler;
   ```

3. Add memory-toc dependency to `crates/memory-topics/Cargo.toml`:
   ```toml
   # Add to [dependencies]
   memory-toc = { workspace = true }
   ```
  </action>
  <verify>
    cd /Users/richardhightower/clients/spillwave/src/agent-memory && cargo test -p memory-topics labeling
  </verify>
  <done>
    TopicLabeler with LLM integration. Keyword fallback when LLM unavailable. label_topics_batch for efficiency. All tests pass.
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/richardhightower/clients/spillwave/src/agent-memory

# Crate compiles
cargo build -p memory-topics

# All labeling and keyword tests pass
cargo test -p memory-topics keywords
cargo test -p memory-topics labeling

# Clippy clean
cargo clippy -p memory-topics -- -D warnings
```
</verification>

<success_criteria>
- [ ] keywords.rs implements extract_keywords with stopword filtering
- [ ] label_from_keywords generates capitalized labels
- [ ] truncate_label handles max length constraint
- [ ] TopicLabeler supports both LLM and keyword-only modes
- [ ] LLM labeling uses existing Summarizer trait
- [ ] Fallback to keywords when LLM fails (configurable)
- [ ] Batch labeling handles multiple topics efficiently
- [ ] All tests pass
- [ ] No clippy warnings
</success_criteria>

<output>
After completion, create `.planning/phases/14-topic-graph-memory/14-02-SUMMARY.md`
</output>
