---
phase: 14-topic-graph-memory
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Cargo.toml
  - crates/memory-topics/Cargo.toml
  - crates/memory-topics/src/lib.rs
  - crates/memory-topics/src/error.rs
  - crates/memory-topics/src/types.rs
  - crates/memory-topics/src/config.rs
  - crates/memory-topics/src/storage.rs
  - crates/memory-topics/src/similarity.rs
  - crates/memory-topics/src/extraction.rs
  - crates/memory-storage/src/column_families.rs
autonomous: true

must_haves:
  truths:
    - "Topics can be extracted from TOC node embeddings using HDBSCAN clustering"
    - "Topics are stored in CF_TOPICS column family with ULID identifiers"
    - "Topic-node links stored in CF_TOPIC_LINKS enable bidirectional lookup"
    - "Topic extraction is optional and disabled by default"
    - "Cosine similarity between embeddings can be computed without external dependencies"
  artifacts:
    - path: "crates/memory-topics/Cargo.toml"
      provides: "Topic crate dependencies"
      contains: "hdbscan"
    - path: "crates/memory-topics/src/types.rs"
      provides: "Topic, TopicLink, TopicRelationship structs"
      exports: ["Topic", "TopicLink", "TopicRelationship", "TopicStatus", "RelationshipType"]
    - path: "crates/memory-topics/src/storage.rs"
      provides: "RocksDB storage operations for topics"
      exports: ["TopicStorage", "save_topic", "get_topic", "list_topics"]
    - path: "crates/memory-topics/src/extraction.rs"
      provides: "HDBSCAN-based topic extraction"
      exports: ["TopicExtractor", "extract_topics"]
    - path: "crates/memory-topics/src/similarity.rs"
      provides: "Pure Rust vector similarity functions"
      exports: ["cosine_similarity", "calculate_centroid"]
  key_links:
    - from: "crates/memory-topics/src/extraction.rs"
      to: "hdbscan"
      via: "density-based clustering"
      pattern: "Hdbscan::new"
    - from: "crates/memory-topics/src/storage.rs"
      to: "memory-storage"
      via: "column family operations"
      pattern: "Storage::put.*CF_TOPICS"
---

<objective>
Create the memory-topics crate with topic data types, storage layer, and HDBSCAN-based extraction.

Purpose: Establish the foundation for semantic topic discovery. Topics are conceptual themes extracted from TOC summaries that enable agents to explore memory by concept rather than just time.

Output: New memory-topics crate with Topic/TopicLink types, CF_TOPICS/CF_TOPIC_LINKS column families, HDBSCAN clustering, and cosine similarity utilities.
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/14-topic-graph-memory/14-RESEARCH.md
@Cargo.toml
@crates/memory-storage/src/column_families.rs
@crates/memory-types/src/lib.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create memory-topics crate structure with types and config</name>
  <files>
    crates/memory-topics/Cargo.toml
    crates/memory-topics/src/lib.rs
    crates/memory-topics/src/error.rs
    crates/memory-topics/src/types.rs
    crates/memory-topics/src/config.rs
    Cargo.toml
  </files>
  <action>
Create the new memory-topics crate:

1. Create `crates/memory-topics/Cargo.toml`:
   - Package name: memory-topics
   - Inherit workspace.package fields (version, edition, license, repository)
   - Dependencies:
     - hdbscan = "0.12"
     - memory-types = { workspace = true }
     - memory-storage = { workspace = true }
     - tokio = { workspace = true, features = ["rt"] }
     - tracing = { workspace = true }
     - thiserror = { workspace = true }
     - serde = { workspace = true, features = ["derive"] }
     - serde_json = { workspace = true }
     - chrono = { workspace = true, features = ["serde"] }
     - ulid = { workspace = true }
   - Dev dependencies:
     - tempfile = { workspace = true }
     - tokio = { workspace = true, features = ["test-util", "macros", "rt-multi-thread"] }

2. Create `crates/memory-topics/src/lib.rs`:
   ```rust
   //! # memory-topics
   //!
   //! Semantic topic extraction and management for Agent Memory.
   //!
   //! This crate enables conceptual discovery through topics extracted from
   //! TOC summaries using embedding clustering. Topics have time-decayed
   //! importance scores and can form relationships (similar, parent/child).
   //!
   //! ## Features
   //! - HDBSCAN clustering for automatic topic detection
   //! - Time-decayed importance scoring
   //! - Topic relationships (similar, parent, child)
   //! - Optional feature - disabled by default
   //!
   //! ## Requirements
   //! - TOPIC-01: Topic extraction from TOC summaries
   //! - TOPIC-02: Topics stored in CF_TOPICS
   //! - TOPIC-07: Optional via configuration
   //! - TOPIC-08: GetTopicGraphStatus RPC for discovery

   pub mod config;
   pub mod error;
   pub mod extraction;
   pub mod similarity;
   pub mod storage;
   pub mod types;

   pub use config::TopicsConfig;
   pub use error::TopicsError;
   pub use extraction::TopicExtractor;
   pub use similarity::{calculate_centroid, cosine_similarity};
   pub use storage::TopicStorage;
   pub use types::{RelationshipType, Topic, TopicLink, TopicRelationship, TopicStatus};
   ```

3. Create `crates/memory-topics/src/error.rs`:
   ```rust
   //! Topic error types.

   use thiserror::Error;

   /// Errors that can occur during topic operations.
   #[derive(Debug, Error)]
   pub enum TopicsError {
       /// Storage error
       #[error("Storage error: {0}")]
       Storage(#[from] memory_storage::StorageError),

       /// Clustering error
       #[error("Clustering error: {0}")]
       Clustering(String),

       /// Serialization error
       #[error("Serialization error: {0}")]
       Serialization(#[from] serde_json::Error),

       /// Topic not found
       #[error("Topic not found: {0}")]
       NotFound(String),

       /// Invalid configuration
       #[error("Invalid configuration: {0}")]
       InvalidConfig(String),

       /// Feature disabled
       #[error("Topic graph is disabled")]
       Disabled,

       /// Embedding error
       #[error("Embedding error: {0}")]
       Embedding(String),

       /// Invalid input
       #[error("Invalid input: {0}")]
       InvalidInput(String),

       /// Cycle detected in relationships
       #[error("Cycle detected in topic relationships")]
       CycleDetected,
   }
   ```

4. Create `crates/memory-topics/src/types.rs`:
   ```rust
   //! Topic data types.

   use chrono::{DateTime, Utc};
   use serde::{Deserialize, Serialize};

   /// A semantic topic extracted from TOC summaries.
   #[derive(Debug, Clone, Serialize, Deserialize)]
   pub struct Topic {
       /// Unique identifier (ULID)
       pub topic_id: String,
       /// Human-readable label (max 50 chars)
       pub label: String,
       /// Centroid embedding for similarity matching
       pub embedding: Vec<f32>,
       /// Time-decayed importance score
       pub importance_score: f64,
       /// Number of linked TOC nodes
       pub node_count: u32,
       /// First occurrence timestamp
       pub created_at: DateTime<Utc>,
       /// Most recent mention timestamp
       pub last_mentioned_at: DateTime<Utc>,
       /// Active or pruned status
       pub status: TopicStatus,
       /// Keywords extracted from cluster
       pub keywords: Vec<String>,
   }

   impl Topic {
       /// Create a new topic with default values.
       pub fn new(topic_id: String, label: String, embedding: Vec<f32>) -> Self {
           let now = Utc::now();
           Self {
               topic_id,
               label,
               embedding,
               importance_score: 1.0,
               node_count: 0,
               created_at: now,
               last_mentioned_at: now,
               status: TopicStatus::Active,
               keywords: Vec::new(),
           }
       }

       /// Check if topic is active.
       pub fn is_active(&self) -> bool {
           self.status == TopicStatus::Active
       }
   }

   /// Topic status.
   #[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
   pub enum TopicStatus {
       /// Topic is active and visible
       Active,
       /// Topic has been pruned due to inactivity
       Pruned,
   }

   /// Link between a topic and a TOC node.
   #[derive(Debug, Clone, Serialize, Deserialize)]
   pub struct TopicLink {
       /// Topic identifier
       pub topic_id: String,
       /// TOC node identifier
       pub node_id: String,
       /// Relevance score (0.0 - 1.0)
       pub relevance: f32,
       /// When the link was created
       pub created_at: DateTime<Utc>,
   }

   impl TopicLink {
       /// Create a new topic-node link.
       pub fn new(topic_id: String, node_id: String, relevance: f32) -> Self {
           Self {
               topic_id,
               node_id,
               relevance,
               created_at: Utc::now(),
           }
       }
   }

   /// Type of relationship between topics.
   #[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
   pub enum RelationshipType {
       /// Topics are semantically similar
       Similar,
       /// This topic is broader (parent)
       Parent,
       /// This topic is narrower (child)
       Child,
   }

   impl RelationshipType {
       /// Get short code for storage key.
       pub fn code(&self) -> &'static str {
           match self {
               RelationshipType::Similar => "sim",
               RelationshipType::Parent => "par",
               RelationshipType::Child => "chi",
           }
       }

       /// Parse from code.
       pub fn from_code(code: &str) -> Option<Self> {
           match code {
               "sim" => Some(RelationshipType::Similar),
               "par" => Some(RelationshipType::Parent),
               "chi" => Some(RelationshipType::Child),
               _ => None,
           }
       }
   }

   /// Relationship between two topics.
   #[derive(Debug, Clone, Serialize, Deserialize)]
   pub struct TopicRelationship {
       /// Source topic ID
       pub from_topic_id: String,
       /// Target topic ID
       pub to_topic_id: String,
       /// Type of relationship
       pub relationship_type: RelationshipType,
       /// Strength of relationship (0.0 - 1.0)
       pub score: f32,
   }

   impl TopicRelationship {
       /// Create a new relationship.
       pub fn new(
           from_topic_id: String,
           to_topic_id: String,
           relationship_type: RelationshipType,
           score: f32,
       ) -> Self {
           Self {
               from_topic_id,
               to_topic_id,
               relationship_type,
               score,
           }
       }
   }

   /// Statistics about the topic graph.
   #[derive(Debug, Clone, Default, Serialize, Deserialize)]
   pub struct TopicStats {
       /// Total number of topics
       pub topic_count: u64,
       /// Number of topic-node links
       pub link_count: u64,
       /// Number of topic relationships
       pub relationship_count: u64,
       /// Timestamp of last extraction (ms since epoch)
       pub last_extraction_ms: i64,
       /// Configured half-life in days
       pub half_life_days: u32,
       /// Configured similarity threshold
       pub similarity_threshold: f32,
   }

   #[cfg(test)]
   mod tests {
       use super::*;

       #[test]
       fn test_topic_new() {
           let topic = Topic::new(
               "01HRQ7D5KQ".to_string(),
               "Test Topic".to_string(),
               vec![0.1, 0.2, 0.3],
           );
           assert!(topic.is_active());
           assert_eq!(topic.node_count, 0);
           assert!((topic.importance_score - 1.0).abs() < f64::EPSILON);
       }

       #[test]
       fn test_relationship_type_code() {
           assert_eq!(RelationshipType::Similar.code(), "sim");
           assert_eq!(RelationshipType::Parent.code(), "par");
           assert_eq!(RelationshipType::Child.code(), "chi");
           assert_eq!(
               RelationshipType::from_code("sim"),
               Some(RelationshipType::Similar)
           );
       }
   }
   ```

5. Create `crates/memory-topics/src/config.rs`:
   ```rust
   //! Topic configuration.

   use serde::{Deserialize, Serialize};

   /// Master configuration for topic functionality.
   #[derive(Debug, Clone, Serialize, Deserialize)]
   pub struct TopicsConfig {
       /// Master switch - topics disabled by default
       #[serde(default)]
       pub enabled: bool,

       /// Extraction settings
       #[serde(default)]
       pub extraction: ExtractionConfig,

       /// Labeling settings
       #[serde(default)]
       pub labeling: LabelingConfig,

       /// Importance scoring settings
       #[serde(default)]
       pub importance: ImportanceConfig,

       /// Relationship detection settings
       #[serde(default)]
       pub relationships: RelationshipsConfig,

       /// Lifecycle management settings
       #[serde(default)]
       pub lifecycle: LifecycleConfig,
   }

   impl Default for TopicsConfig {
       fn default() -> Self {
           Self {
               enabled: false, // Disabled by default
               extraction: ExtractionConfig::default(),
               labeling: LabelingConfig::default(),
               importance: ImportanceConfig::default(),
               relationships: RelationshipsConfig::default(),
               lifecycle: LifecycleConfig::default(),
           }
       }
   }

   /// Topic extraction configuration.
   #[derive(Debug, Clone, Serialize, Deserialize)]
   pub struct ExtractionConfig {
       /// Minimum cluster size for HDBSCAN
       #[serde(default = "default_min_cluster_size")]
       pub min_cluster_size: usize,

       /// Minimum similarity threshold for cluster membership
       #[serde(default = "default_similarity_threshold")]
       pub similarity_threshold: f32,

       /// Cron schedule for extraction job
       #[serde(default = "default_extraction_schedule")]
       pub schedule: String,

       /// Maximum nodes to process per batch
       #[serde(default = "default_batch_size")]
       pub batch_size: usize,
   }

   impl Default for ExtractionConfig {
       fn default() -> Self {
           Self {
               min_cluster_size: default_min_cluster_size(),
               similarity_threshold: default_similarity_threshold(),
               schedule: default_extraction_schedule(),
               batch_size: default_batch_size(),
           }
       }
   }

   fn default_min_cluster_size() -> usize {
       3
   }
   fn default_similarity_threshold() -> f32 {
       0.75
   }
   fn default_extraction_schedule() -> String {
       "0 4 * * *".to_string() // 4 AM daily
   }
   fn default_batch_size() -> usize {
       500
   }

   /// Topic labeling configuration.
   #[derive(Debug, Clone, Serialize, Deserialize)]
   pub struct LabelingConfig {
       /// Whether to use LLM for labeling
       #[serde(default = "default_true")]
       pub use_llm: bool,

       /// Fall back to keyword extraction if LLM fails
       #[serde(default = "default_true")]
       pub fallback_to_keywords: bool,

       /// Maximum label length
       #[serde(default = "default_max_label_length")]
       pub max_label_length: usize,

       /// Number of top keywords to extract
       #[serde(default = "default_top_keywords")]
       pub top_keywords: usize,
   }

   impl Default for LabelingConfig {
       fn default() -> Self {
           Self {
               use_llm: default_true(),
               fallback_to_keywords: default_true(),
               max_label_length: default_max_label_length(),
               top_keywords: default_top_keywords(),
           }
       }
   }

   fn default_true() -> bool {
       true
   }
   fn default_max_label_length() -> usize {
       50
   }
   fn default_top_keywords() -> usize {
       5
   }

   /// Importance scoring configuration.
   #[derive(Debug, Clone, Serialize, Deserialize)]
   pub struct ImportanceConfig {
       /// Half-life in days for decay
       #[serde(default = "default_half_life_days")]
       pub half_life_days: u32,

       /// Boost multiplier for mentions within 7 days
       #[serde(default = "default_recency_boost")]
       pub recency_boost: f64,
   }

   impl Default for ImportanceConfig {
       fn default() -> Self {
           Self {
               half_life_days: default_half_life_days(),
               recency_boost: default_recency_boost(),
           }
       }
   }

   fn default_half_life_days() -> u32 {
       30
   }
   fn default_recency_boost() -> f64 {
       2.0
   }

   /// Relationship detection configuration.
   #[derive(Debug, Clone, Serialize, Deserialize)]
   pub struct RelationshipsConfig {
       /// Minimum similarity for "similar" relationship
       #[serde(default = "default_similar_threshold")]
       pub similar_threshold: f32,

       /// Maximum hierarchy depth
       #[serde(default = "default_max_hierarchy_depth")]
       pub max_hierarchy_depth: usize,

       /// Enable parent/child detection
       #[serde(default = "default_true")]
       pub enable_hierarchy: bool,
   }

   impl Default for RelationshipsConfig {
       fn default() -> Self {
           Self {
               similar_threshold: default_similar_threshold(),
               max_hierarchy_depth: default_max_hierarchy_depth(),
               enable_hierarchy: default_true(),
           }
       }
   }

   fn default_similar_threshold() -> f32 {
       0.8
   }
   fn default_max_hierarchy_depth() -> usize {
       3
   }

   /// Lifecycle management configuration.
   #[derive(Debug, Clone, Serialize, Deserialize)]
   pub struct LifecycleConfig {
       /// Days of inactivity before pruning
       #[serde(default = "default_prune_after_days")]
       pub prune_after_days: u32,

       /// Cron schedule for pruning job
       #[serde(default = "default_prune_schedule")]
       pub prune_schedule: String,

       /// Enable automatic resurrection on re-mention
       #[serde(default = "default_true")]
       pub auto_resurrect: bool,
   }

   impl Default for LifecycleConfig {
       fn default() -> Self {
           Self {
               prune_after_days: default_prune_after_days(),
               prune_schedule: default_prune_schedule(),
               auto_resurrect: default_true(),
           }
       }
   }

   fn default_prune_after_days() -> u32 {
       90
   }
   fn default_prune_schedule() -> String {
       "0 5 * * 0".to_string() // 5 AM Sunday
   }

   #[cfg(test)]
   mod tests {
       use super::*;

       #[test]
       fn test_default_config_disabled() {
           let config = TopicsConfig::default();
           assert!(!config.enabled);
       }

       #[test]
       fn test_extraction_defaults() {
           let config = ExtractionConfig::default();
           assert_eq!(config.min_cluster_size, 3);
           assert!((config.similarity_threshold - 0.75).abs() < f32::EPSILON);
       }
   }
   ```

6. Update workspace `Cargo.toml`:
   - Add "crates/memory-topics" to workspace members
   - Add memory-topics = { path = "crates/memory-topics" } to workspace.dependencies
   - Add hdbscan = "0.12" to workspace.dependencies
  </action>
  <verify>
    cd /Users/richardhightower/clients/spillwave/src/agent-memory && cargo check -p memory-topics
  </verify>
  <done>
    Crate compiles. Types (Topic, TopicLink, TopicRelationship) defined. Config with defaults. Error types defined.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement similarity functions and HDBSCAN extraction</name>
  <files>
    crates/memory-topics/src/similarity.rs
    crates/memory-topics/src/extraction.rs
  </files>
  <action>
Implement pure Rust similarity and HDBSCAN clustering:

1. Create `crates/memory-topics/src/similarity.rs`:
   ```rust
   //! Vector similarity functions.
   //!
   //! Pure Rust implementations without external dependencies.

   /// Calculate cosine similarity between two vectors.
   ///
   /// Returns value in [-1.0, 1.0] where 1.0 = identical direction.
   ///
   /// # Panics
   /// Panics if vectors have different dimensions.
   pub fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
       assert_eq!(a.len(), b.len(), "Vectors must have same dimension");

       let dot_product: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
       let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
       let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();

       if norm_a == 0.0 || norm_b == 0.0 {
           return 0.0;
       }

       dot_product / (norm_a * norm_b)
   }

   /// Calculate the centroid of multiple embeddings.
   ///
   /// Returns a normalized vector representing the center of the cluster.
   pub fn calculate_centroid(embeddings: &[&[f32]]) -> Vec<f32> {
       if embeddings.is_empty() {
           return Vec::new();
       }

       let dim = embeddings[0].len();
       let n = embeddings.len() as f32;
       let mut centroid = vec![0.0f32; dim];

       for embedding in embeddings {
           assert_eq!(embedding.len(), dim, "All embeddings must have same dimension");
           for (i, &val) in embedding.iter().enumerate() {
               centroid[i] += val;
           }
       }

       // Average
       for val in centroid.iter_mut() {
           *val /= n;
       }

       // Normalize
       normalize(&mut centroid);

       centroid
   }

   /// Normalize a vector to unit length in place.
   pub fn normalize(v: &mut [f32]) {
       let norm: f32 = v.iter().map(|x| x * x).sum::<f32>().sqrt();
       if norm > 0.0 {
           for val in v.iter_mut() {
               *val /= norm;
           }
       }
   }

   /// Calculate pairwise distances between embeddings.
   ///
   /// Returns a distance matrix where distance = 1 - cosine_similarity.
   pub fn pairwise_distances(embeddings: &[Vec<f32>]) -> Vec<Vec<f64>> {
       let n = embeddings.len();
       let mut distances = vec![vec![0.0f64; n]; n];

       for i in 0..n {
           for j in (i + 1)..n {
               let sim = cosine_similarity(&embeddings[i], &embeddings[j]);
               let dist = (1.0 - sim) as f64;
               distances[i][j] = dist;
               distances[j][i] = dist;
           }
       }

       distances
   }

   #[cfg(test)]
   mod tests {
       use super::*;

       #[test]
       fn test_cosine_similarity_identical() {
           let a = vec![1.0, 0.0, 0.0];
           let b = vec![1.0, 0.0, 0.0];
           assert!((cosine_similarity(&a, &b) - 1.0).abs() < 0.001);
       }

       #[test]
       fn test_cosine_similarity_orthogonal() {
           let a = vec![1.0, 0.0];
           let b = vec![0.0, 1.0];
           assert!(cosine_similarity(&a, &b).abs() < 0.001);
       }

       #[test]
       fn test_cosine_similarity_opposite() {
           let a = vec![1.0, 0.0];
           let b = vec![-1.0, 0.0];
           assert!((cosine_similarity(&a, &b) + 1.0).abs() < 0.001);
       }

       #[test]
       fn test_cosine_similarity_similar() {
           let a = vec![0.8, 0.6];
           let b = vec![0.6, 0.8];
           let sim = cosine_similarity(&a, &b);
           assert!(sim > 0.9); // Should be similar
       }

       #[test]
       fn test_calculate_centroid() {
           let e1 = vec![1.0, 0.0, 0.0];
           let e2 = vec![0.0, 1.0, 0.0];
           let embeddings: Vec<&[f32]> = vec![&e1, &e2];
           let centroid = calculate_centroid(&embeddings);
           // Average of [1,0,0] and [0,1,0] = [0.5, 0.5, 0] normalized
           let expected_norm = (0.5f32.powi(2) * 2.0).sqrt();
           assert!((centroid[0] - 0.5 / expected_norm).abs() < 0.001);
           assert!((centroid[1] - 0.5 / expected_norm).abs() < 0.001);
           assert!(centroid[2].abs() < 0.001);
       }

       #[test]
       fn test_normalize() {
           let mut v = vec![3.0, 4.0];
           normalize(&mut v);
           assert!((v[0] - 0.6).abs() < 0.001);
           assert!((v[1] - 0.8).abs() < 0.001);
       }

       #[test]
       fn test_pairwise_distances() {
           let embeddings = vec![
               vec![1.0, 0.0],
               vec![0.0, 1.0],
               vec![1.0, 0.0],
           ];
           let distances = pairwise_distances(&embeddings);
           assert!((distances[0][2]).abs() < 0.001); // Identical
           assert!((distances[0][1] - 1.0).abs() < 0.001); // Orthogonal
       }
   }
   ```

2. Create `crates/memory-topics/src/extraction.rs`:
   ```rust
   //! Topic extraction using HDBSCAN clustering.

   use hdbscan::{Hdbscan, HdbscanHyperParams};
   use tracing::{debug, info, warn};
   use ulid::Ulid;

   use crate::config::ExtractionConfig;
   use crate::error::TopicsError;
   use crate::similarity::calculate_centroid;
   use crate::types::Topic;

   /// Input for topic extraction: node ID with its embedding.
   #[derive(Debug, Clone)]
   pub struct NodeEmbedding {
       /// TOC node identifier
       pub node_id: String,
       /// Embedding vector
       pub embedding: Vec<f32>,
       /// Summary text (for keyword extraction)
       pub summary: String,
   }

   /// Result of clustering: cluster ID to node IDs.
   #[derive(Debug)]
   pub struct ClusterResult {
       /// Cluster label (-1 = noise)
       pub label: i32,
       /// Node IDs in this cluster
       pub node_ids: Vec<String>,
       /// Node embeddings in this cluster
       pub embeddings: Vec<Vec<f32>>,
       /// Summaries for keyword extraction
       pub summaries: Vec<String>,
   }

   /// Topic extractor using HDBSCAN clustering.
   pub struct TopicExtractor {
       config: ExtractionConfig,
   }

   impl TopicExtractor {
       /// Create a new topic extractor.
       pub fn new(config: ExtractionConfig) -> Self {
           Self { config }
       }

       /// Cluster embeddings using HDBSCAN.
       ///
       /// Returns clusters grouped by label. Label -1 indicates noise points.
       pub fn cluster(&self, nodes: &[NodeEmbedding]) -> Result<Vec<ClusterResult>, TopicsError> {
           if nodes.len() < self.config.min_cluster_size {
               debug!(
                   count = nodes.len(),
                   min = self.config.min_cluster_size,
                   "Not enough nodes for clustering"
               );
               return Ok(vec![]);
           }

           info!(count = nodes.len(), "Starting HDBSCAN clustering");

           // Convert to 2D array format expected by hdbscan (f64)
           let data: Vec<Vec<f64>> = nodes
               .iter()
               .map(|n| n.embedding.iter().map(|&x| x as f64).collect())
               .collect();

           // Create clusterer with custom params
           let params = HdbscanHyperParams::builder()
               .min_cluster_size(self.config.min_cluster_size)
               .build();

           let clusterer = Hdbscan::new(&data, params);

           // Run clustering (blocking - should be called from spawn_blocking)
           let labels = clusterer
               .cluster()
               .map_err(|e| TopicsError::Clustering(e.to_string()))?;

           info!(
               labels = labels.len(),
               unique = count_unique_clusters(&labels),
               "Clustering complete"
           );

           // Group nodes by cluster label
           let clusters = group_by_cluster(nodes, &labels);

           Ok(clusters)
       }

       /// Create topics from cluster results.
       ///
       /// Each cluster becomes a topic with a centroid embedding.
       pub fn create_topics(&self, clusters: &[ClusterResult]) -> Vec<Topic> {
           clusters
               .iter()
               .filter(|c| c.label >= 0) // Skip noise
               .map(|cluster| {
                   let topic_id = Ulid::new().to_string();

                   // Calculate centroid
                   let embedding_refs: Vec<&[f32]> =
                       cluster.embeddings.iter().map(|e| e.as_slice()).collect();
                   let centroid = calculate_centroid(&embedding_refs);

                   // Create topic with placeholder label (labeling is Plan 14-02)
                   let mut topic = Topic::new(
                       topic_id,
                       format!("Topic {}", cluster.label), // Placeholder
                       centroid,
                   );
                   topic.node_count = cluster.node_ids.len() as u32;

                   topic
               })
               .collect()
       }

       /// Get configuration.
       pub fn config(&self) -> &ExtractionConfig {
           &self.config
       }
   }

   /// Count unique non-noise clusters.
   fn count_unique_clusters(labels: &[i32]) -> usize {
       let mut unique: std::collections::HashSet<i32> = labels.iter().copied().collect();
       unique.remove(&-1); // Remove noise label
       unique.len()
   }

   /// Group nodes by cluster label.
   fn group_by_cluster(nodes: &[NodeEmbedding], labels: &[i32]) -> Vec<ClusterResult> {
       use std::collections::HashMap;

       let mut groups: HashMap<i32, ClusterResult> = HashMap::new();

       for (node, &label) in nodes.iter().zip(labels.iter()) {
           let cluster = groups.entry(label).or_insert_with(|| ClusterResult {
               label,
               node_ids: Vec::new(),
               embeddings: Vec::new(),
               summaries: Vec::new(),
           });
           cluster.node_ids.push(node.node_id.clone());
           cluster.embeddings.push(node.embedding.clone());
           cluster.summaries.push(node.summary.clone());
       }

       groups.into_values().collect()
   }

   #[cfg(test)]
   mod tests {
       use super::*;

       fn make_node(id: &str, embedding: Vec<f32>) -> NodeEmbedding {
           NodeEmbedding {
               node_id: id.to_string(),
               embedding,
               summary: format!("Summary for {}", id),
           }
       }

       #[test]
       fn test_cluster_insufficient_nodes() {
           let config = ExtractionConfig {
               min_cluster_size: 5,
               ..Default::default()
           };
           let extractor = TopicExtractor::new(config);

           let nodes = vec![
               make_node("n1", vec![1.0, 0.0, 0.0]),
               make_node("n2", vec![0.0, 1.0, 0.0]),
           ];

           let result = extractor.cluster(&nodes).unwrap();
           assert!(result.is_empty());
       }

       #[test]
       fn test_count_unique_clusters() {
           let labels = vec![0, 0, 1, 1, -1, 2, -1];
           assert_eq!(count_unique_clusters(&labels), 3); // 0, 1, 2
       }

       #[test]
       fn test_group_by_cluster() {
           let nodes = vec![
               make_node("n1", vec![1.0]),
               make_node("n2", vec![2.0]),
               make_node("n3", vec![3.0]),
           ];
           let labels = vec![0, 0, 1];

           let groups = group_by_cluster(&nodes, &labels);
           assert_eq!(groups.len(), 2);

           let cluster_0 = groups.iter().find(|c| c.label == 0).unwrap();
           assert_eq!(cluster_0.node_ids.len(), 2);

           let cluster_1 = groups.iter().find(|c| c.label == 1).unwrap();
           assert_eq!(cluster_1.node_ids.len(), 1);
       }

       #[test]
       fn test_create_topics_skips_noise() {
           let config = ExtractionConfig::default();
           let extractor = TopicExtractor::new(config);

           let clusters = vec![
               ClusterResult {
                   label: -1, // Noise
                   node_ids: vec!["n1".to_string()],
                   embeddings: vec![vec![1.0, 0.0]],
                   summaries: vec!["Summary".to_string()],
               },
               ClusterResult {
                   label: 0,
                   node_ids: vec!["n2".to_string(), "n3".to_string()],
                   embeddings: vec![vec![1.0, 0.0], vec![0.0, 1.0]],
                   summaries: vec!["Sum1".to_string(), "Sum2".to_string()],
               },
           ];

           let topics = extractor.create_topics(&clusters);
           assert_eq!(topics.len(), 1); // Only non-noise cluster
           assert_eq!(topics[0].node_count, 2);
       }
   }
   ```
  </action>
  <verify>
    cd /Users/richardhightower/clients/spillwave/src/agent-memory && cargo test -p memory-topics -- --include-ignored 2>/dev/null || cargo test -p memory-topics
  </verify>
  <done>
    cosine_similarity and calculate_centroid implemented in pure Rust. HDBSCAN clustering via TopicExtractor. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement topic storage with new column families</name>
  <files>
    crates/memory-topics/src/storage.rs
    crates/memory-storage/src/column_families.rs
  </files>
  <action>
Implement topic storage operations:

1. Update `crates/memory-storage/src/column_families.rs` to add new column families:
   ```rust
   // Add these constants (DO NOT replace existing content, ADD to it):

   /// Column family for topic records
   pub const CF_TOPICS: &str = "topics";

   /// Column family for topic-node links
   pub const CF_TOPIC_LINKS: &str = "topic_links";

   /// Column family for topic relationships
   pub const CF_TOPIC_RELS: &str = "topic_rels";
   ```

   Also update the ALL_CFS array to include the new column families.

2. Create `crates/memory-topics/src/storage.rs`:
   ```rust
   //! Topic storage operations.
   //!
   //! Manages topics, links, and relationships in RocksDB column families.

   use std::sync::Arc;

   use memory_storage::Storage;
   use tracing::{debug, instrument};

   use crate::error::TopicsError;
   use crate::types::{
       RelationshipType, Topic, TopicLink, TopicRelationship, TopicStats, TopicStatus,
   };

   /// Column family names (must match memory-storage)
   pub const CF_TOPICS: &str = "topics";
   pub const CF_TOPIC_LINKS: &str = "topic_links";
   pub const CF_TOPIC_RELS: &str = "topic_rels";

   /// Key format for topics: topic:{topic_id}
   pub fn topic_key(topic_id: &str) -> String {
       format!("topic:{}", topic_id)
   }

   /// Key format for topic links: link:{topic_id}:{node_id}
   pub fn topic_link_key(topic_id: &str, node_id: &str) -> String {
       format!("link:{}:{}", topic_id, node_id)
   }

   /// Secondary index: node:{node_id}:{topic_id}
   pub fn node_topic_key(node_id: &str, topic_id: &str) -> String {
       format!("node:{}:{}", node_id, topic_id)
   }

   /// Key format for relationships: rel:{from}:{type}:{to}
   pub fn relationship_key(from_id: &str, rel_type: &str, to_id: &str) -> String {
       format!("rel:{}:{}:{}", from_id, rel_type, to_id)
   }

   /// Topic storage interface.
   pub struct TopicStorage {
       storage: Arc<Storage>,
   }

   impl TopicStorage {
       /// Create a new topic storage wrapper.
       pub fn new(storage: Arc<Storage>) -> Self {
           Self { storage }
       }

       /// Get underlying storage.
       pub fn storage(&self) -> &Arc<Storage> {
           &self.storage
       }

       // --- Topic CRUD ---

       /// Save a topic.
       #[instrument(skip(self, topic), fields(topic_id = %topic.topic_id))]
       pub fn save_topic(&self, topic: &Topic) -> Result<(), TopicsError> {
           let key = topic_key(&topic.topic_id);
           let value = serde_json::to_vec(topic)?;
           self.storage.put(CF_TOPICS, key.as_bytes(), &value)?;
           debug!("Saved topic");
           Ok(())
       }

       /// Get a topic by ID.
       #[instrument(skip(self))]
       pub fn get_topic(&self, topic_id: &str) -> Result<Option<Topic>, TopicsError> {
           let key = topic_key(topic_id);
           match self.storage.get(CF_TOPICS, key.as_bytes())? {
               Some(bytes) => {
                   let topic: Topic = serde_json::from_slice(&bytes)?;
                   Ok(Some(topic))
               }
               None => Ok(None),
           }
       }

       /// Delete a topic and its links.
       #[instrument(skip(self))]
       pub fn delete_topic(&self, topic_id: &str) -> Result<(), TopicsError> {
           // Delete topic
           let key = topic_key(topic_id);
           self.storage.delete(CF_TOPICS, key.as_bytes())?;

           // Delete links (would need iteration in production)
           // For now, links are cleaned up separately
           debug!("Deleted topic");
           Ok(())
       }

       /// List all active topics.
       pub fn list_topics(&self) -> Result<Vec<Topic>, TopicsError> {
           let prefix = b"topic:";
           let mut topics = Vec::new();

           for (_, value) in self.storage.prefix_iterator(CF_TOPICS, prefix)? {
               let topic: Topic = serde_json::from_slice(&value)?;
               if topic.status == TopicStatus::Active {
                   topics.push(topic);
               }
           }

           Ok(topics)
       }

       /// List topics sorted by importance score (descending).
       pub fn list_topics_by_importance(&self, limit: usize) -> Result<Vec<Topic>, TopicsError> {
           let mut topics = self.list_topics()?;
           topics.sort_by(|a, b| {
               b.importance_score
                   .partial_cmp(&a.importance_score)
                   .unwrap_or(std::cmp::Ordering::Equal)
           });
           topics.truncate(limit);
           Ok(topics)
       }

       // --- Topic Links ---

       /// Save a topic-node link.
       #[instrument(skip(self, link), fields(topic_id = %link.topic_id, node_id = %link.node_id))]
       pub fn save_link(&self, link: &TopicLink) -> Result<(), TopicsError> {
           let value = serde_json::to_vec(link)?;

           // Primary key: topic -> nodes
           let primary_key = topic_link_key(&link.topic_id, &link.node_id);
           self.storage
               .put(CF_TOPIC_LINKS, primary_key.as_bytes(), &value)?;

           // Secondary key: node -> topics
           let secondary_key = node_topic_key(&link.node_id, &link.topic_id);
           self.storage
               .put(CF_TOPIC_LINKS, secondary_key.as_bytes(), &value)?;

           debug!("Saved topic link");
           Ok(())
       }

       /// Get links for a topic.
       pub fn get_links_for_topic(&self, topic_id: &str) -> Result<Vec<TopicLink>, TopicsError> {
           let prefix = format!("link:{}:", topic_id);
           let mut links = Vec::new();

           for (_, value) in self.storage.prefix_iterator(CF_TOPIC_LINKS, prefix.as_bytes())? {
               let link: TopicLink = serde_json::from_slice(&value)?;
               links.push(link);
           }

           // Sort by relevance descending
           links.sort_by(|a, b| {
               b.relevance
                   .partial_cmp(&a.relevance)
                   .unwrap_or(std::cmp::Ordering::Equal)
           });

           Ok(links)
       }

       /// Get topics for a node.
       pub fn get_topics_for_node(&self, node_id: &str) -> Result<Vec<TopicLink>, TopicsError> {
           let prefix = format!("node:{}:", node_id);
           let mut links = Vec::new();

           for (_, value) in self.storage.prefix_iterator(CF_TOPIC_LINKS, prefix.as_bytes())? {
               let link: TopicLink = serde_json::from_slice(&value)?;
               links.push(link);
           }

           Ok(links)
       }

       // --- Relationships ---

       /// Save a topic relationship.
       #[instrument(skip(self, rel), fields(from = %rel.from_topic_id, to = %rel.to_topic_id))]
       pub fn save_relationship(&self, rel: &TopicRelationship) -> Result<(), TopicsError> {
           let key = relationship_key(
               &rel.from_topic_id,
               rel.relationship_type.code(),
               &rel.to_topic_id,
           );
           let value = serde_json::to_vec(rel)?;
           self.storage.put(CF_TOPIC_RELS, key.as_bytes(), &value)?;
           debug!("Saved relationship");
           Ok(())
       }

       /// Get relationships for a topic.
       pub fn get_relationships(
           &self,
           topic_id: &str,
           rel_type: Option<RelationshipType>,
       ) -> Result<Vec<TopicRelationship>, TopicsError> {
           let prefix = match rel_type {
               Some(rt) => format!("rel:{}:{}:", topic_id, rt.code()),
               None => format!("rel:{}:", topic_id),
           };

           let mut rels = Vec::new();

           for (_, value) in self.storage.prefix_iterator(CF_TOPIC_RELS, prefix.as_bytes())? {
               let rel: TopicRelationship = serde_json::from_slice(&value)?;
               rels.push(rel);
           }

           // Sort by score descending
           rels.sort_by(|a, b| {
               b.score
                   .partial_cmp(&a.score)
                   .unwrap_or(std::cmp::Ordering::Equal)
           });

           Ok(rels)
       }

       // --- Statistics ---

       /// Get topic graph statistics.
       pub fn get_stats(&self) -> Result<TopicStats, TopicsError> {
           let topics = self.list_topics()?;
           let topic_count = topics.len() as u64;

           // Count links (approximate via prefix scan)
           let link_count = self
               .storage
               .prefix_iterator(CF_TOPIC_LINKS, b"link:")?
               .count() as u64;

           // Count relationships
           let relationship_count = self
               .storage
               .prefix_iterator(CF_TOPIC_RELS, b"rel:")?
               .count() as u64;

           Ok(TopicStats {
               topic_count,
               link_count,
               relationship_count,
               last_extraction_ms: 0, // Set by extraction job
               half_life_days: 30,    // From config
               similarity_threshold: 0.75,
           })
       }
   }

   #[cfg(test)]
   mod tests {
       use super::*;

       #[test]
       fn test_topic_key() {
           assert_eq!(topic_key("abc123"), "topic:abc123");
       }

       #[test]
       fn test_topic_link_key() {
           assert_eq!(topic_link_key("t1", "n1"), "link:t1:n1");
       }

       #[test]
       fn test_node_topic_key() {
           assert_eq!(node_topic_key("n1", "t1"), "node:n1:t1");
       }

       #[test]
       fn test_relationship_key() {
           assert_eq!(relationship_key("t1", "sim", "t2"), "rel:t1:sim:t2");
       }
   }
   ```
  </action>
  <verify>
    cd /Users/richardhightower/clients/spillwave/src/agent-memory && cargo build -p memory-topics && cargo test -p memory-topics
  </verify>
  <done>
    TopicStorage with CRUD operations for topics, links, relationships. Key formats defined. Column families added to memory-storage. All tests pass.
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/richardhightower/clients/spillwave/src/agent-memory

# Crate compiles
cargo build -p memory-topics

# All unit tests pass
cargo test -p memory-topics

# Clippy clean
cargo clippy -p memory-topics -- -D warnings

# Verify workspace includes memory-topics
grep -q "memory-topics" Cargo.toml && echo "memory-topics in workspace"

# Verify hdbscan dependency
grep -q "hdbscan" Cargo.toml && echo "hdbscan dependency added"
```
</verification>

<success_criteria>
- [ ] memory-topics crate exists in workspace
- [ ] hdbscan dependency added to workspace
- [ ] Topic, TopicLink, TopicRelationship types defined
- [ ] TopicsConfig with extraction, labeling, importance, relationships, lifecycle sections
- [ ] cosine_similarity and calculate_centroid implemented in pure Rust
- [ ] TopicExtractor uses HDBSCAN for clustering
- [ ] TopicStorage provides CRUD for topics, links, relationships
- [ ] CF_TOPICS, CF_TOPIC_LINKS, CF_TOPIC_RELS column families defined
- [ ] All unit tests pass
- [ ] No clippy warnings
</success_criteria>

<output>
After completion, create `.planning/phases/14-topic-graph-memory/14-01-SUMMARY.md`
</output>
