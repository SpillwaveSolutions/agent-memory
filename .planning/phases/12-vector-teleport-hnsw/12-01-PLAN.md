---
phase: 12-vector-teleport-hnsw
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Cargo.toml
  - crates/memory-embeddings/Cargo.toml
  - crates/memory-embeddings/src/lib.rs
  - crates/memory-embeddings/src/error.rs
  - crates/memory-embeddings/src/model.rs
  - crates/memory-embeddings/src/candle.rs
  - crates/memory-embeddings/src/cache.rs
autonomous: true

must_haves:
  truths:
    - "Embeddings can be generated for any text input"
    - "Model files are cached locally after first download"
    - "Batch embedding processes multiple texts efficiently"
    - "Embedding dimension is 384 (all-MiniLM-L6-v2)"
  artifacts:
    - path: "crates/memory-embeddings/Cargo.toml"
      provides: "Embedding crate dependencies"
      contains: "candle-core"
    - path: "crates/memory-embeddings/src/model.rs"
      provides: "EmbeddingModel trait definition"
      exports: ["EmbeddingModel", "Embedding"]
    - path: "crates/memory-embeddings/src/candle.rs"
      provides: "Candle-based implementation"
      exports: ["CandleEmbedder"]
    - path: "crates/memory-embeddings/src/cache.rs"
      provides: "Model file caching"
      exports: ["ModelCache", "get_or_download_model"]
  key_links:
    - from: "crates/memory-embeddings/src/candle.rs"
      to: "candle-transformers"
      via: "loads sentence-transformer model"
      pattern: "BertModel::load"
    - from: "crates/memory-embeddings/src/cache.rs"
      to: "huggingface-hub"
      via: "downloads model files"
      pattern: "hf_hub::api::sync::Api"
---

<objective>
Create the memory-embeddings crate with Candle-based local embedding generation.

Purpose: Enable semantic vector generation from text without external API dependencies. This is the foundation for vector search - all TOC summaries and grips will be embedded for similarity matching.

Output: New memory-embeddings crate with EmbeddingModel trait, CandleEmbedder implementation, and model file caching.
</objective>

<execution_context>
@/Users/richardhightower/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardhightower/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/12-vector-teleport-hnsw/12-RESEARCH.md
@Cargo.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create memory-embeddings crate structure</name>
  <files>
    crates/memory-embeddings/Cargo.toml
    crates/memory-embeddings/src/lib.rs
    crates/memory-embeddings/src/error.rs
    Cargo.toml
  </files>
  <action>
Create the new memory-embeddings crate:

1. Create `crates/memory-embeddings/Cargo.toml`:
   - Package name: memory-embeddings
   - Inherit workspace.package fields (version, edition, license, repository)
   - Dependencies:
     - candle-core = "0.8"
     - candle-nn = "0.8"
     - candle-transformers = "0.8"
     - tokenizers = "0.20"
     - hf-hub = "0.3" (for model downloading)
     - memory-types = { workspace = true }
     - tokio = { workspace = true }
     - tracing = { workspace = true }
     - thiserror = { workspace = true }
     - serde = { workspace = true }
   - Dev dependencies:
     - tempfile = { workspace = true }
     - tokio = { workspace = true, features = ["test-util", "macros", "rt-multi-thread"] }

2. Create `crates/memory-embeddings/src/lib.rs`:
   ```rust
   //! # memory-embeddings
   //!
   //! Local embedding generation for Agent Memory using Candle.
   //!
   //! This crate provides semantic vector embeddings for TOC summaries and grip
   //! excerpts, enabling similarity search without external API calls.
   //!
   //! ## Features
   //! - Local inference via Candle (no Python, no API)
   //! - all-MiniLM-L6-v2 model (384 dimensions)
   //! - Automatic model file caching
   //! - Batch embedding for efficiency
   //!
   //! ## Requirements
   //! - FR-01: Local embedding via Candle
   //! - No external API dependencies
   //! - Works offline after initial model download

   pub mod error;
   pub mod model;
   pub mod candle;
   pub mod cache;

   pub use error::EmbeddingError;
   pub use model::{EmbeddingModel, Embedding};
   pub use candle::CandleEmbedder;
   pub use cache::{ModelCache, get_or_download_model};
   ```

3. Create `crates/memory-embeddings/src/error.rs`:
   ```rust
   //! Embedding error types.

   use thiserror::Error;

   /// Errors that can occur during embedding operations.
   #[derive(Debug, Error)]
   pub enum EmbeddingError {
       /// Candle model error
       #[error("Candle error: {0}")]
       Candle(#[from] candle_core::Error),

       /// Tokenizer error
       #[error("Tokenizer error: {0}")]
       Tokenizer(String),

       /// Model file not found
       #[error("Model file not found: {0}")]
       ModelNotFound(String),

       /// Download error
       #[error("Failed to download model: {0}")]
       Download(String),

       /// IO error
       #[error("IO error: {0}")]
       Io(#[from] std::io::Error),

       /// Cache error
       #[error("Cache error: {0}")]
       Cache(String),

       /// Invalid input
       #[error("Invalid input: {0}")]
       InvalidInput(String),

       /// Dimension mismatch
       #[error("Dimension mismatch: expected {expected}, got {actual}")]
       DimensionMismatch { expected: usize, actual: usize },
   }
   ```

4. Update workspace `Cargo.toml`:
   - Add "crates/memory-embeddings" to workspace members
   - Add memory-embeddings = { path = "crates/memory-embeddings" } to workspace.dependencies
   - Add candle-core = "0.8" to workspace.dependencies
   - Add candle-nn = "0.8" to workspace.dependencies
   - Add candle-transformers = "0.8" to workspace.dependencies
   - Add tokenizers = "0.20" to workspace.dependencies
   - Add hf-hub = "0.3" to workspace.dependencies
  </action>
  <verify>
    cd /Users/richardhightower/clients/spillwave/src/agent-memory && cargo check -p memory-embeddings
  </verify>
  <done>
    Crate compiles, error types defined, workspace includes memory-embeddings and Candle dependencies
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement EmbeddingModel trait and types</name>
  <files>
    crates/memory-embeddings/src/model.rs
  </files>
  <action>
Create the embedding model trait definition:

1. Create `crates/memory-embeddings/src/model.rs`:
   ```rust
   //! Embedding model trait and types.
   //!
   //! Defines the interface for generating vector embeddings from text.

   use crate::error::EmbeddingError;

   /// Vector embedding - a normalized float array.
   #[derive(Debug, Clone)]
   pub struct Embedding {
       /// The embedding vector (normalized to unit length)
       pub values: Vec<f32>,
   }

   impl Embedding {
       /// Create a new embedding from a vector.
       /// Normalizes the vector to unit length.
       pub fn new(values: Vec<f32>) -> Self {
           let norm: f32 = values.iter().map(|x| x * x).sum::<f32>().sqrt();
           let normalized = if norm > 0.0 {
               values.iter().map(|x| x / norm).collect()
           } else {
               values
           };
           Self { values: normalized }
       }

       /// Create embedding without normalization (for pre-normalized vectors)
       pub fn from_normalized(values: Vec<f32>) -> Self {
           Self { values }
       }

       /// Get the embedding dimension
       pub fn dimension(&self) -> usize {
           self.values.len()
       }

       /// Compute cosine similarity with another embedding.
       /// Returns value in [-1, 1] range (1 = identical).
       pub fn cosine_similarity(&self, other: &Embedding) -> f32 {
           if self.values.len() != other.values.len() {
               return 0.0;
           }
           // Since both are normalized, dot product = cosine similarity
           self.values.iter()
               .zip(other.values.iter())
               .map(|(a, b)| a * b)
               .sum()
       }
   }

   /// Model information
   #[derive(Debug, Clone)]
   pub struct ModelInfo {
       /// Model name (e.g., "all-MiniLM-L6-v2")
       pub name: String,
       /// Embedding dimension
       pub dimension: usize,
       /// Maximum sequence length in tokens
       pub max_sequence_length: usize,
   }

   /// Trait for embedding models.
   ///
   /// Implementations must be thread-safe (Send + Sync) for concurrent use.
   pub trait EmbeddingModel: Send + Sync {
       /// Get model information
       fn info(&self) -> &ModelInfo;

       /// Generate embedding for a single text.
       fn embed(&self, text: &str) -> Result<Embedding, EmbeddingError>;

       /// Generate embeddings for multiple texts (batch).
       /// Default implementation calls embed() for each text.
       fn embed_batch(&self, texts: &[&str]) -> Result<Vec<Embedding>, EmbeddingError> {
           texts.iter()
               .map(|text| self.embed(text))
               .collect()
       }

       /// Generate embeddings for multiple owned strings.
       fn embed_texts(&self, texts: &[String]) -> Result<Vec<Embedding>, EmbeddingError> {
           let refs: Vec<&str> = texts.iter().map(|s| s.as_str()).collect();
           self.embed_batch(&refs)
       }
   }

   #[cfg(test)]
   mod tests {
       use super::*;

       #[test]
       fn test_embedding_normalization() {
           let emb = Embedding::new(vec![3.0, 4.0]);
           // 3-4-5 triangle: normalized should be [0.6, 0.8]
           assert!((emb.values[0] - 0.6).abs() < 0.001);
           assert!((emb.values[1] - 0.8).abs() < 0.001);
       }

       #[test]
       fn test_cosine_similarity_identical() {
           let emb1 = Embedding::new(vec![1.0, 0.0, 0.0]);
           let emb2 = Embedding::new(vec![1.0, 0.0, 0.0]);
           assert!((emb1.cosine_similarity(&emb2) - 1.0).abs() < 0.001);
       }

       #[test]
       fn test_cosine_similarity_orthogonal() {
           let emb1 = Embedding::new(vec![1.0, 0.0]);
           let emb2 = Embedding::new(vec![0.0, 1.0]);
           assert!(emb1.cosine_similarity(&emb2).abs() < 0.001);
       }

       #[test]
       fn test_cosine_similarity_opposite() {
           let emb1 = Embedding::new(vec![1.0, 0.0]);
           let emb2 = Embedding::new(vec![-1.0, 0.0]);
           assert!((emb1.cosine_similarity(&emb2) + 1.0).abs() < 0.001);
       }
   }
   ```
  </action>
  <verify>
    cd /Users/richardhightower/clients/spillwave/src/agent-memory && cargo test -p memory-embeddings model
  </verify>
  <done>
    EmbeddingModel trait defined. Embedding type with normalization and similarity. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement model cache and CandleEmbedder</name>
  <files>
    crates/memory-embeddings/src/cache.rs
    crates/memory-embeddings/src/candle.rs
  </files>
  <action>
Implement model caching and Candle embedding:

1. Create `crates/memory-embeddings/src/cache.rs`:
   ```rust
   //! Model file caching.
   //!
   //! Downloads and caches model files from HuggingFace Hub.

   use std::path::{Path, PathBuf};
   use tracing::{debug, info};

   use crate::error::EmbeddingError;

   /// Default model repository on HuggingFace
   pub const DEFAULT_MODEL_REPO: &str = "sentence-transformers/all-MiniLM-L6-v2";

   /// Required model files
   pub const MODEL_FILES: &[&str] = &[
       "config.json",
       "tokenizer.json",
       "model.safetensors",
   ];

   /// Model cache configuration
   #[derive(Debug, Clone)]
   pub struct ModelCache {
       /// Cache directory path
       pub cache_dir: PathBuf,
       /// Model repository ID
       pub repo_id: String,
   }

   impl Default for ModelCache {
       fn default() -> Self {
           let cache_dir = dirs::cache_dir()
               .unwrap_or_else(|| PathBuf::from(".cache"))
               .join("agent-memory")
               .join("models");

           Self {
               cache_dir,
               repo_id: DEFAULT_MODEL_REPO.to_string(),
           }
       }
   }

   impl ModelCache {
       /// Create a new model cache with custom settings
       pub fn new(cache_dir: impl Into<PathBuf>, repo_id: impl Into<String>) -> Self {
           Self {
               cache_dir: cache_dir.into(),
               repo_id: repo_id.into(),
           }
       }

       /// Get the model directory path
       pub fn model_dir(&self) -> PathBuf {
           self.cache_dir.join(self.repo_id.replace('/', "_"))
       }

       /// Check if all model files are cached
       pub fn is_cached(&self) -> bool {
           let model_dir = self.model_dir();
           MODEL_FILES.iter().all(|f| model_dir.join(f).exists())
       }

       /// Get path to a specific model file
       pub fn file_path(&self, filename: &str) -> PathBuf {
           self.model_dir().join(filename)
       }
   }

   /// Get or download model files.
   ///
   /// Returns paths to config.json, tokenizer.json, and model.safetensors.
   pub fn get_or_download_model(cache: &ModelCache) -> Result<ModelPaths, EmbeddingError> {
       let model_dir = cache.model_dir();

       if cache.is_cached() {
           debug!(path = ?model_dir, "Using cached model");
       } else {
           info!(repo = %cache.repo_id, "Downloading model files...");
           download_model_files(cache)?;
       }

       Ok(ModelPaths {
           config: model_dir.join("config.json"),
           tokenizer: model_dir.join("tokenizer.json"),
           weights: model_dir.join("model.safetensors"),
       })
   }

   /// Paths to model files
   #[derive(Debug, Clone)]
   pub struct ModelPaths {
       pub config: PathBuf,
       pub tokenizer: PathBuf,
       pub weights: PathBuf,
   }

   /// Download model files from HuggingFace Hub
   fn download_model_files(cache: &ModelCache) -> Result<(), EmbeddingError> {
       use hf_hub::api::sync::Api;

       let api = Api::new().map_err(|e| EmbeddingError::Download(e.to_string()))?;
       let repo = api.model(cache.repo_id.clone());

       std::fs::create_dir_all(cache.model_dir())?;

       for filename in MODEL_FILES {
           info!(file = filename, "Downloading...");
           let source_path = repo.get(filename)
               .map_err(|e| EmbeddingError::Download(format!("{}: {}", filename, e)))?;

           let dest_path = cache.file_path(filename);
           std::fs::copy(&source_path, &dest_path)?;
           debug!(file = filename, "Downloaded to {:?}", dest_path);
       }

       Ok(())
   }

   #[cfg(test)]
   mod tests {
       use super::*;
       use tempfile::TempDir;

       #[test]
       fn test_cache_default() {
           let cache = ModelCache::default();
           assert!(cache.cache_dir.to_string_lossy().contains("agent-memory"));
           assert_eq!(cache.repo_id, DEFAULT_MODEL_REPO);
       }

       #[test]
       fn test_is_cached_empty() {
           let temp = TempDir::new().unwrap();
           let cache = ModelCache::new(temp.path(), "test/model");
           assert!(!cache.is_cached());
       }
   }
   ```

2. Create `crates/memory-embeddings/src/candle.rs`:
   ```rust
   //! Candle-based embedding implementation.
   //!
   //! Uses all-MiniLM-L6-v2 for 384-dimensional embeddings.

   use std::sync::Arc;

   use candle_core::{DType, Device, Tensor};
   use candle_nn::VarBuilder;
   use candle_transformers::models::bert::{BertModel, Config as BertConfig};
   use tokenizers::Tokenizer;
   use tracing::{debug, info};

   use crate::cache::{get_or_download_model, ModelCache};
   use crate::error::EmbeddingError;
   use crate::model::{Embedding, EmbeddingModel, ModelInfo};

   /// Embedding dimension for all-MiniLM-L6-v2
   pub const EMBEDDING_DIM: usize = 384;

   /// Maximum sequence length
   pub const MAX_SEQ_LENGTH: usize = 256;

   /// Default batch size for embedding
   pub const DEFAULT_BATCH_SIZE: usize = 32;

   /// Candle-based embedder using all-MiniLM-L6-v2.
   pub struct CandleEmbedder {
       model: BertModel,
       tokenizer: Tokenizer,
       device: Device,
       info: ModelInfo,
   }

   impl CandleEmbedder {
       /// Load the embedding model from cache (downloading if needed).
       pub fn load(cache: &ModelCache) -> Result<Self, EmbeddingError> {
           let paths = get_or_download_model(cache)?;
           Self::load_from_paths(&paths.config, &paths.tokenizer, &paths.weights)
       }

       /// Load with default cache settings
       pub fn load_default() -> Result<Self, EmbeddingError> {
           let cache = ModelCache::default();
           Self::load(&cache)
       }

       /// Load from explicit file paths
       pub fn load_from_paths(
           config_path: &std::path::Path,
           tokenizer_path: &std::path::Path,
           weights_path: &std::path::Path,
       ) -> Result<Self, EmbeddingError> {
           info!("Loading embedding model...");

           // Use CPU device (GPU support can be added later with feature flags)
           let device = Device::Cpu;

           // Load config
           let config_str = std::fs::read_to_string(config_path)?;
           let config: BertConfig = serde_json::from_str(&config_str)
               .map_err(|e| EmbeddingError::ModelNotFound(format!("Invalid config: {}", e)))?;

           // Load tokenizer
           let tokenizer = Tokenizer::from_file(tokenizer_path)
               .map_err(|e| EmbeddingError::Tokenizer(e.to_string()))?;

           // Load model weights
           let vb = unsafe {
               VarBuilder::from_mmaped_safetensors(
                   &[weights_path.to_path_buf()],
                   DType::F32,
                   &device,
               )?
           };

           let model = BertModel::load(vb, &config)?;

           info!(
               dim = EMBEDDING_DIM,
               max_seq = MAX_SEQ_LENGTH,
               "Model loaded successfully"
           );

           Ok(Self {
               model,
               tokenizer,
               device,
               info: ModelInfo {
                   name: "all-MiniLM-L6-v2".to_string(),
                   dimension: EMBEDDING_DIM,
                   max_sequence_length: MAX_SEQ_LENGTH,
               },
           })
       }

       /// Mean pooling over token embeddings (excluding padding)
       fn mean_pooling(&self, embeddings: &Tensor, attention_mask: &Tensor) -> Result<Tensor, EmbeddingError> {
           // Expand attention mask to embedding dimension
           let mask = attention_mask.unsqueeze(2)?.broadcast_as(embeddings.shape())?;
           let mask_f32 = mask.to_dtype(DType::F32)?;

           // Masked sum
           let masked = embeddings.broadcast_mul(&mask_f32)?;
           let sum = masked.sum(1)?;

           // Divide by sum of mask (number of real tokens)
           let mask_sum = mask_f32.sum(1)?;
           let mask_sum = mask_sum.clamp(1e-9, f64::MAX)?; // Avoid division by zero

           let mean = sum.broadcast_div(&mask_sum)?;
           Ok(mean)
       }
   }

   impl EmbeddingModel for CandleEmbedder {
       fn info(&self) -> &ModelInfo {
           &self.info
       }

       fn embed(&self, text: &str) -> Result<Embedding, EmbeddingError> {
           let embeddings = self.embed_batch(&[text])?;
           Ok(embeddings.into_iter().next().unwrap())
       }

       fn embed_batch(&self, texts: &[&str]) -> Result<Vec<Embedding>, EmbeddingError> {
           if texts.is_empty() {
               return Ok(vec![]);
           }

           debug!(count = texts.len(), "Embedding batch");

           // Tokenize all texts
           let encodings = self.tokenizer
               .encode_batch(texts.to_vec(), true)
               .map_err(|e| EmbeddingError::Tokenizer(e.to_string()))?;

           // Pad to same length
           let max_len = encodings.iter()
               .map(|e| e.get_ids().len())
               .max()
               .unwrap_or(0)
               .min(MAX_SEQ_LENGTH);

           let mut input_ids: Vec<Vec<u32>> = Vec::new();
           let mut attention_masks: Vec<Vec<u32>> = Vec::new();

           for encoding in &encodings {
               let ids = encoding.get_ids();
               let mask = encoding.get_attention_mask();

               let truncated_len = ids.len().min(max_len);
               let mut padded_ids = ids[..truncated_len].to_vec();
               let mut padded_mask = mask[..truncated_len].to_vec();

               // Pad to max_len
               padded_ids.resize(max_len, 0);
               padded_mask.resize(max_len, 0);

               input_ids.push(padded_ids);
               attention_masks.push(padded_mask);
           }

           // Convert to tensors
           let batch_size = texts.len();
           let input_ids_flat: Vec<u32> = input_ids.into_iter().flatten().collect();
           let mask_flat: Vec<u32> = attention_masks.into_iter().flatten().collect();

           let input_ids = Tensor::from_vec(input_ids_flat, (batch_size, max_len), &self.device)?;
           let attention_mask = Tensor::from_vec(mask_flat, (batch_size, max_len), &self.device)?;
           let token_type_ids = Tensor::zeros_like(&input_ids)?;

           // Forward pass
           let output = self.model.forward(&input_ids, &token_type_ids, Some(&attention_mask))?;

           // Mean pooling
           let pooled = self.mean_pooling(&output, &attention_mask)?;

           // Convert to embeddings
           let pooled_vec: Vec<Vec<f32>> = pooled.to_vec2()?;

           let embeddings: Vec<Embedding> = pooled_vec
               .into_iter()
               .map(Embedding::new) // Normalizes the vector
               .collect();

           debug!(count = embeddings.len(), dim = EMBEDDING_DIM, "Batch complete");

           Ok(embeddings)
       }
   }

   #[cfg(test)]
   mod tests {
       use super::*;

       // Note: Integration tests require model download, run with:
       // cargo test -p memory-embeddings --features integration -- --ignored

       #[test]
       #[ignore = "requires model download"]
       fn test_load_model() {
           let embedder = CandleEmbedder::load_default().unwrap();
           assert_eq!(embedder.info().dimension, EMBEDDING_DIM);
       }

       #[test]
       #[ignore = "requires model download"]
       fn test_embed_single() {
           let embedder = CandleEmbedder::load_default().unwrap();
           let emb = embedder.embed("Hello, world!").unwrap();
           assert_eq!(emb.dimension(), EMBEDDING_DIM);
       }

       #[test]
       #[ignore = "requires model download"]
       fn test_embed_batch() {
           let embedder = CandleEmbedder::load_default().unwrap();
           let texts = vec!["Hello", "World", "Test"];
           let embeddings = embedder.embed_batch(&texts).unwrap();
           assert_eq!(embeddings.len(), 3);
           for emb in &embeddings {
               assert_eq!(emb.dimension(), EMBEDDING_DIM);
           }
       }

       #[test]
       #[ignore = "requires model download"]
       fn test_similar_texts_high_similarity() {
           let embedder = CandleEmbedder::load_default().unwrap();
           let emb1 = embedder.embed("The cat sat on the mat").unwrap();
           let emb2 = embedder.embed("A cat is sitting on a mat").unwrap();
           let emb3 = embedder.embed("Python programming language").unwrap();

           let sim_similar = emb1.cosine_similarity(&emb2);
           let sim_different = emb1.cosine_similarity(&emb3);

           // Similar sentences should have higher similarity
           assert!(sim_similar > sim_different);
           assert!(sim_similar > 0.7); // Should be quite similar
       }
   }
   ```
  </action>
  <verify>
    cd /Users/richardhightower/clients/spillwave/src/agent-memory && cargo build -p memory-embeddings && cargo test -p memory-embeddings
  </verify>
  <done>
    CandleEmbedder loads model and generates embeddings. ModelCache handles file caching. Unit tests pass (integration tests require model download).
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/richardhightower/clients/spillwave/src/agent-memory

# Crate compiles
cargo build -p memory-embeddings

# Unit tests pass
cargo test -p memory-embeddings

# Clippy clean
cargo clippy -p memory-embeddings -- -D warnings

# Integration test (requires model download, ~200MB)
# cargo test -p memory-embeddings -- --ignored
```
</verification>

<success_criteria>
- [ ] memory-embeddings crate exists in workspace
- [ ] Candle dependencies added to workspace (candle-core, candle-nn, candle-transformers)
- [ ] tokenizers and hf-hub dependencies added
- [ ] EmbeddingModel trait defined with embed() and embed_batch()
- [ ] Embedding type with normalization and cosine_similarity
- [ ] CandleEmbedder implements EmbeddingModel using all-MiniLM-L6-v2
- [ ] ModelCache handles model file caching
- [ ] get_or_download_model downloads from HuggingFace Hub
- [ ] Unit tests pass
- [ ] No clippy warnings
</success_criteria>

<output>
After completion, create `.planning/phases/12-vector-teleport-hnsw/12-01-SUMMARY.md`
</output>
